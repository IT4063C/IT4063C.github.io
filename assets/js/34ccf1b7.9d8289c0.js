"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[4128],{17918:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var a=n(87462),l=(n(67294),n(3905));n(8209);const r={sidebar_position:1,title:"Machine Learning - Classification",draft:!1},i=void 0,o={unversionedId:"classification/index",id:"classification/index",title:"Machine Learning - Classification",description:"This module, we'll talk about another class of problems we can solve with machine learning. Classification is a subcategory of supervised learning where the goal is to predict the categorical class labels of new instances based on past observations.",source:"@site/content/course-notes/11.classification/index.md",sourceDirName:"11.classification",slug:"/classification/",permalink:"/course-notes/classification/",draft:!1,editUrl:"https://github.com/IT4063C/IT4063C.github.io/tree/main/content/course-notes/11.classification/index.md",tags:[],version:"current",lastUpdatedBy:"Yahya Gilany",lastUpdatedAt:1668362328,formattedLastUpdatedAt:"Nov 13, 2022",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Machine Learning - Classification",draft:!1},sidebar:"notes",previous:{title:"Regression",permalink:"/course-notes/ml-regression/"}},s={},d=[{value:"1\ufe0f\u20e3. Ask",id:"1\ufe0f\u20e3-ask",level:2},{value:"2\ufe0f\u20e3. Prepare",id:"2\ufe0f\u20e3-prepare",level:2},{value:"1. EDA",id:"1-eda",level:3},{value:"2. Data Splitting",id:"2-data-splitting",level:3},{value:"3\ufe0f\u20e3. Process",id:"3\ufe0f\u20e3-process",level:2},{value:"1. Dropping Features",id:"1-dropping-features",level:3},{value:"2. Imputing Missing Values",id:"2-imputing-missing-values",level:3},{value:"The Why...",id:"the-why",level:4},{value:"The How...",id:"the-how",level:4},{value:"3. Standardization and Normalization",id:"3-standardization-and-normalization",level:3},{value:"The Why...",id:"the-why-1",level:4},{value:"The How...",id:"the-how-1",level:4},{value:"4. Handling Categorical Features",id:"4-handling-categorical-features",level:3},{value:"4\ufe0f\u20e3. Analyze",id:"4\ufe0f\u20e3-analyze",level:2},{value:"Dummy Classifier",id:"dummy-classifier",level:3},{value:"Logistic Classifier",id:"logistic-classifier",level:3},{value:"5\ufe0f\u20e3. Deploy",id:"5\ufe0f\u20e3-deploy",level:2}],p=t=>function(e){return console.warn("Component "+t+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",e)},m=p("HTMLOutputBlock"),c=p("CodeOutputBlock"),h={toc:d};function u(t){let{components:e,...r}=t;return(0,l.kt)("wrapper",(0,a.Z)({},h,r,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("p",null,"This module, we'll talk about another class of problems we can solve with machine learning. Classification is a subcategory of supervised learning where the goal is to predict the categorical class labels of new instances based on past observations."),(0,l.kt)("p",null,"Generally, there are 2 main types of classification problems:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Binary: classifying  into 2 classes. (i.e., Spam and not-spam)"),(0,l.kt)("li",{parentName:"ul"},"Multi-class classification: classifying into multiple classes. (i.e., images of handwritten character recognition)")),(0,l.kt)("p",null,"in it's simplest formats, the objective is to find a line or a plane that segregates the data points into different classes.\n",(0,l.kt)("img",{alt:"simple classification",src:n(50526).Z,width:"397",height:"373"})),(0,l.kt)("p",null,"In this notebook, I'm doing what we should do in every project: have a single cell that imports all the libraries we need. This way, we can easily import all the libraries we need in one cell and we can easily see what libraries we need for this project."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Python \u22653.10 is required\nimport sys\nassert sys.version_info >= (3, 10)\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport os\n\n# Scikit Learn imports\n# For the pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n# For preprocessing\nfrom sklearn.preprocessing import (\n  OneHotEncoder,\n  OrdinalEncoder,\n  StandardScaler\n)\nfrom sklearn.impute import (\n  SimpleImputer\n)\n# For model selection\nfrom sklearn.model_selection import (\n  StratifiedShuffleSplit,\n  train_test_split,\n  cross_val_score,\n  KFold,\n  GridSearchCV\n)\n\n# Classifier Algorithms\nfrom sklearn import metrics\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# To save and load models\nimport pickle\n\n# To plot pretty figures\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nplt.style.use(\"bmh\")\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n")),(0,l.kt)("h1",{id:"titanic-survival"},"Titanic Survival"),(0,l.kt)("p",null,"In this tutorial we'll use the titanic dataset from ",(0,l.kt)("a",{parentName:"p",href:"https://www.encyclopedia-titanica.org/"},"Encyclopedia Titanica"),". "),(0,l.kt)("h2",{id:"1\ufe0f\u20e3-ask"},"1\ufe0f\u20e3. Ask"),(0,l.kt)("p",null,"We will use to train a model that classifies passengers based on whether or not they've survived. The objective would be to develop a model that would predict survival based on other features and attributes."),(0,l.kt)("p",null,"We'll start with importing the dataset from ",(0,l.kt)("inlineCode",{parentName:"p"},"data/titanic.csv"),", and printing samples of the data."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"titanic = pd.read_csv('data/titanic.csv')\ntitanic.head()\n")),(0,l.kt)(m,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>pclass</th>\n      <th>survived</th>\n      <th>name</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>ticket</th>\n      <th>fare</th>\n      <th>cabin</th>\n      <th>embarked</th>\n      <th>boat</th>\n      <th>body</th>\n      <th>home.dest</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Allen, Miss. Elisabeth Walton</td>\n      <td>female</td>\n      <td>29.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24160</td>\n      <td>211.3375</td>\n      <td>B5</td>\n      <td>S</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>St Louis, MO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Allison, Master. Hudson Trevor</td>\n      <td>male</td>\n      <td>0.92</td>\n      <td>1</td>\n      <td>2</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n      <td>11</td>\n      <td>NaN</td>\n      <td>Montreal, PQ / Chesterville, ON</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Allison, Miss. Helen Loraine</td>\n      <td>female</td>\n      <td>2.00</td>\n      <td>1</td>\n      <td>2</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Montreal, PQ / Chesterville, ON</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Allison, Mr. Hudson Joshua Creighton</td>\n      <td>male</td>\n      <td>30.00</td>\n      <td>1</td>\n      <td>2</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n      <td>NaN</td>\n      <td>135.0</td>\n      <td>Montreal, PQ / Chesterville, ON</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n      <td>female</td>\n      <td>25.00</td>\n      <td>1</td>\n      <td>2</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Montreal, PQ / Chesterville, ON</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("p",null,"We see that we have the following features, this info was provided on the ",(0,l.kt)("a",{parentName:"p",href:"https://www.encyclopedia-titanica.org/"},"Encyclopedia Titanica"),"."),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Feature"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"pclass")),(0,l.kt)("td",{parentName:"tr",align:null},"Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd). This can be considered as proxy for socio-economic status.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"survived")),(0,l.kt)("td",{parentName:"tr",align:null},"Survival (0 = No; 1 = Yes)")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"name")),(0,l.kt)("td",{parentName:"tr",align:null},"Name")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"sex")),(0,l.kt)("td",{parentName:"tr",align:null},"Sex")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"age")),(0,l.kt)("td",{parentName:"tr",align:null},"Age")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"sibsp")),(0,l.kt)("td",{parentName:"tr",align:null},"Number of Siblings/Spouses Aboard")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"parch")),(0,l.kt)("td",{parentName:"tr",align:null},"Number of Parents/Children Aboard")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"ticket")),(0,l.kt)("td",{parentName:"tr",align:null},"Ticket Number")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"fare")),(0,l.kt)("td",{parentName:"tr",align:null},"Passenger Fare")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"cabin")),(0,l.kt)("td",{parentName:"tr",align:null},"Cabin")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"embarked")),(0,l.kt)("td",{parentName:"tr",align:null},"Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"boat")),(0,l.kt)("td",{parentName:"tr",align:null},"Lifeboat (if survived)")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"body")),(0,l.kt)("td",{parentName:"tr",align:null},"Body number (if did not survive and body was recovered)")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"home.dest")),(0,l.kt)("td",{parentName:"tr",align:null},"Home/Destination")))),(0,l.kt)("h2",{id:"2\ufe0f\u20e3-prepare"},"2\ufe0f\u20e3. Prepare"),(0,l.kt)("h3",{id:"1-eda"},"1. EDA"),(0,l.kt)("p",null,"Let's do some exploratory data analysis to get a better understanding of the data. "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"titanic.info()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 1309 entries, 0 to 1308\n    Data columns (total 14 columns):\n     #   Column     Non-Null Count  Dtype  \n    ---  ------     --------------  -----  \n     0   pclass     1309 non-null   int64  \n     1   survived   1309 non-null   int64  \n     2   name       1309 non-null   object \n     3   sex        1309 non-null   object \n     4   age        1046 non-null   float64\n     5   sibsp      1309 non-null   int64  \n     6   parch      1309 non-null   int64  \n     7   ticket     1309 non-null   object \n     8   fare       1308 non-null   float64\n     9   cabin      295 non-null    object \n     10  embarked   1307 non-null   object \n     11  boat       486 non-null    object \n     12  body       121 non-null    float64\n     13  home.dest  745 non-null    object \n    dtypes: float64(3), int64(4), object(7)\n    memory usage: 143.3+ KB\n"))),(0,l.kt)("p",null,"let's visualize the histogram of the data to see the distribution of the data."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# REMEMBER: number of bins indicate the number if intervals we want to divide our data into\ntitanic.hist(bins=50, figsize=(20,15))\nplt.show()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(97020).Z,width:"1606",height:"1221"}))),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\ud83e\udd89 ",(0,l.kt)("strong",{parentName:"p"},"Note"),": Take a moment and try to make some observations based on the above before you continue reading the below.")),(0,l.kt)("p",null,"We can observer the following:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Most of the passengers were in the 3rd class."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"survived")," is the target variable, and we can see that most of the passengers did not survive."),(0,l.kt)("li",{parentName:"ul"},"Majority of passengers were traveling alone (no siblings or spouses, no parents or children)."),(0,l.kt)("li",{parentName:"ul"},"There's a lot of missing data in the ",(0,l.kt)("inlineCode",{parentName:"li"},"cabin"),", and ",(0,l.kt)("inlineCode",{parentName:"li"},"age")," columns."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"pclass"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"sex"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"embarked")," are categorical variables.")),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Column"),(0,l.kt)("th",{parentName:"tr",align:null},"Type"),(0,l.kt)("th",{parentName:"tr",align:null},"notes"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pclass"),(0,l.kt)("td",{parentName:"tr",align:null},"categorical - ordinal"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"name"),(0,l.kt)("td",{parentName:"tr",align:null},"string - would this be valuable to our model?"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"sex"),(0,l.kt)("td",{parentName:"tr",align:null},"categorical - nominal"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"age"),(0,l.kt)("td",{parentName:"tr",align:null},"numerical - continuous"),(0,l.kt)("td",{parentName:"tr",align:null},"the reason why it's not discrete, is that we see the column type set to float64, and some of the values are 0.92 (less than a year old)")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"sibsp"),(0,l.kt)("td",{parentName:"tr",align:null},"numerical - discrete"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"parch"),(0,l.kt)("td",{parentName:"tr",align:null},"numerical - discrete"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ticket"),(0,l.kt)("td",{parentName:"tr",align:null},"string - would this be valuable to our model?"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"fare"),(0,l.kt)("td",{parentName:"tr",align:null},"numerical - continuous"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cabin"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"(Quiz Question)")),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"embarked"),(0,l.kt)("td",{parentName:"tr",align:null},"Categorical - could be ordinal or nominal"),(0,l.kt)("td",{parentName:"tr",align:null},"Do we think people who boarded first, had better chances of survival?")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"boat"),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"boat identifier")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"body"),(0,l.kt)("td",{parentName:"tr",align:null},"numerical (or string)"),(0,l.kt)("td",{parentName:"tr",align:null},"body ID")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"home. dest"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"survived"),(0,l.kt)("td",{parentName:"tr",align:null},"category ",(0,l.kt)("em",{parentName:"td"},"(ordinal, or nominal?)")),(0,l.kt)("td",{parentName:"tr",align:null})))),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"we also realize that both ",(0,l.kt)("inlineCode",{parentName:"li"},"boat")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"body")," are what we call ",(0,l.kt)("strong",{parentName:"li"},"Leaky Features"),". These are features that kind of gives the answer about the future. Having a ",(0,l.kt)("inlineCode",{parentName:"li"},"body")," number, means they didn't survive. These are features that are not available at the time of prediction, and therefore should not be used in the model. We'll drop these features later on.")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"how about missing values?")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"titanic.isnull().sum()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    pclass          0\n    survived        0\n    name            0\n    sex             0\n    age           263\n    sibsp           0\n    parch           0\n    ticket          0\n    fare            1\n    cabin        1014\n    embarked        2\n    boat          823\n    body         1188\n    home.dest     564\n    dtype: int64\n"))),(0,l.kt)("p",null,"We will need to impute the missing values in the Age column."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"BUT")," we shouldn't do any of that until the data is split into training and test sets. We don't want to introduce any bias into the data."),(0,l.kt)("p",null,"Let me show another interesting library called ",(0,l.kt)("inlineCode",{parentName:"p"},"pandas_profiling"),". This library will generate a report of the data, and it's very useful for exploratory data analysis."),(0,l.kt)("p",null,"if you don't already have it installed, you can install it with ",(0,l.kt)("inlineCode",{parentName:"p"},"pipenv install pandas-profiling ipywidgets"),"."),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\ud83e\udd89 ",(0,l.kt)("strong",{parentName:"p"},"Note"),": The report looks like a webpage embedded in the notebook, if you're using Jupyter Lab or VSCode. It may look a bit different if you're viewing this directly on GitHub.")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# pandas_profiling.ProfileReport(titanic, title='Pandas Profiling Report')\n")),(0,l.kt)("p",null,"The generated report is able, to a high degree of correctness, to detect the data types of the columns, and also detect missing values.\nIt's able to show the distribution of the data, and also the correlation between the features."),(0,l.kt)("p",null,"Before we proceed any further, and before we start cleaning, the data, dropping columns, or anything else, we need to split the data into training and test sets."),(0,l.kt)("hr",null),(0,l.kt)("h3",{id:"2-data-splitting"},"2. Data Splitting"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"should we stratify or not?\nWith large datasets, stratified sampling is a good idea. This is because we want to make sure that the training and test sets have the same distribution of the target variable.")),(0,l.kt)("p",null,"in talking with a hypothetical subject-matter expert (SME), they mentioned that when splitting the data, we need:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"all passenger classes needs to be represented in the dataset with the same frequency as they appear in the original dataset."),(0,l.kt)("li",{parentName:"ul"},"They also had a theory that the ",(0,l.kt)("inlineCode",{parentName:"li"},"embarked")," column may have some effect  SME suggested that we should use ",(0,l.kt)("inlineCode",{parentName:"li"},"embarked")," as the stratification feature.")),(0,l.kt)("p",null,"Another SME suggested that ",(0,l.kt)("inlineCode",{parentName:"p"},"sex")," is a more important feature to startify the data split on, and that we should use that instead."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=56)\n\nfor train_index, test_index in split.split(titanic, titanic['embarked']):\n  strat_train_set = titanic.loc[train_index]\n  strat_test_set = titanic.loc[test_index]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_set, test_set = train_test_split(titanic, test_size= 0.2, random_state=45)\n")),(0,l.kt)("p",null,"Let's also separate the features from the labels"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"titanic_X = train_set.drop('survived', axis=1)\ntitanic_y = train_set['survived'].copy()\n")),(0,l.kt)("hr",null),(0,l.kt)("h2",{id:"3\ufe0f\u20e3-process"},"3\ufe0f\u20e3. Process"),(0,l.kt)("h3",{id:"1-dropping-features"},"1. Dropping Features"),(0,l.kt)("p",null,"We generally drop columns that have no signal, or no variance, or not useful for the model. We also drop columns that are ",(0,l.kt)("strong",{parentName:"p"},"Leaky Features"),"."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"name")," column",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"unless we're doing some fancy Natural Language Processing (NLP), or we have some theory about the names may have an effect on the survival rate, (like easier names were easier to remember and call out, and therefore easier to save \ud83e\udd37\u200d\u2642\ufe0f),  we can drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"name")," column."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"ticket")," column",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"we can drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"ticket")," column, because it's just a ticket number, and it's not going to be useful for the model."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"boat")," column",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"we can drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"boat")," column, because it's a leaky feature. if you got on a boat, you survived (very likely)."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"body")," column",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"we can drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"body")," column, because it's a leaky feature. If you have a body number, you didn't survive."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"home.dest")," column",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"we can drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"home.dest")," column, because it's not going to be useful for the model."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"cabin")," column",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"we can drop the ",(0,l.kt)("inlineCode",{parentName:"li"},"cabin")," column, because it's missing 77,5% of the data, and imputing it wouldn't not be effective.")))),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\ud83e\udd89 ",(0,l.kt)("strong",{parentName:"p"},"Remember"),": we separated the dataset into training and test sets, if we're going to drop the columns from one, we're going to remove from the other as well.")),(0,l.kt)("p",null,"Previously, we used the ",(0,l.kt)("inlineCode",{parentName:"p"},".drop()")," method to drop the columns. and we had to call it twice, once for the training set, and once for the test set."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_set.drop(['name', 'ticket', 'boat', 'body', 'home.dest'], axis=1, inplace=True)\ntest_set.drop(['name', 'ticket', 'boat', 'body', 'home.dest'], axis=1, inplace=True)\n")),(0,l.kt)("p",null,"But we've also learned about creating a pipeline. Pipelines allow us to chain together multiple transformations that we can apply to both the training and test sets. "),(0,l.kt)("p",null,"If you remember, we have used the ",(0,l.kt)("inlineCode",{parentName:"p"},"ColumnTransformer")," before to apply different transformation to the ",(0,l.kt)("em",{parentName:"p"},"Numerical")," and ",(0,l.kt)("em",{parentName:"p"},"Categorical")," features. I will also use it this week to drop columns as well."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"We can use the ",(0,l.kt)("inlineCode",{parentName:"strong"},"ColumnTransformer")," to drop columns.")),(0,l.kt)("p",null,"So let's start by creating our full pipeline that we'll use in the data cleaning process."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"drop_columns = ['name', 'ticket', 'boat', 'body', 'home.dest', 'cabin']\n\nfull_pipeline = ColumnTransformer([\n  ('drop_columns', 'drop', drop_columns),\n], remainder='passthrough')\n")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("strong",{parentName:"p"}," \ud83e\udd89 "),"Remember**: Note because the last (technically the only) transformer in the pipeline is not a transformer that supports ",(0,l.kt)("inlineCode",{parentName:"p"},"fit")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"transform")," methods, but a special-cased string, we need to use the ",(0,l.kt)("inlineCode",{parentName:"p"},"passthrough")," keyword for the remainder of data going through the pipeline. ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"},"\ud83d\udcdc Learn more about this here"))),(0,l.kt)("p",null,"to execute the pipeline, we'll use the ",(0,l.kt)("inlineCode",{parentName:"p"},"fit_transform()")," method. This method will execute the pipeline, and return the transformed data as a numpy n-dimensional array."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Apply the Transformation\ntransformed_train_set = full_pipeline.fit_transform(titanic_X)\n\n# Transform the numpy n-dimensional array into a pandas dataframe\ntransformed_train_set = pd.DataFrame(transformed_train_set, columns=titanic_X.columns.drop(drop_columns), index=titanic_X.index)\ntransformed_train_set.head()\n")),(0,l.kt)(m,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>801</th>\n      <td>3</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.75</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>2</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>29.0</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>2</td>\n      <td>female</td>\n      <td>48.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>65.0</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>2</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>3</td>\n      <td>female</td>\n      <td>41.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>20.2125</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("h3",{id:"2-imputing-missing-values"},"2. Imputing Missing Values"),(0,l.kt)("h4",{id:"the-why"},"The Why..."),(0,l.kt)("p",null,"Imputing missing values is a very important step in the data cleaning process. We can't just drop the rows with missing values, because we'll lose a lot of data. Instead we calculate a value that will replace the missing value. This value is called the ",(0,l.kt)("strong",{parentName:"p"},"Imputed Value"),"."),(0,l.kt)("p",null,"Even if there was no missing values in the training set, there may be missing values in the test set, or in new data points. So we need to make sure that we do the imputation when building the training set the imputation is done on both the training and test sets."),(0,l.kt)("h4",{id:"the-how"},"The How..."),(0,l.kt)("p",null,"Generally, we use the ",(0,l.kt)("inlineCode",{parentName:"p"},"median")," value to impute missing values , because it's not affected by outliers. But we can also use the mean value, or the most frequent value."),(0,l.kt)("p",null,"We can use the ",(0,l.kt)("inlineCode",{parentName:"p"},"mean")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"median")," with numerical features, and the ",(0,l.kt)("inlineCode",{parentName:"p"},"most_frequent")," with numerical and categorical features. ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"},"\ud83d\udcdc Learn more about this here")),(0,l.kt)("p",null,"so looks like we need a different imputer for each type of feature (numerical and categorical). So let's let's create 2 lists of feature names, one for numerical features, and one for categorical features. each set will use a different pipeline and a different imputer. "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"num_features = ['age', 'sibsp', 'parch', 'fare']\ncat_features = ['pclass', 'sex', 'embarked']\n\nnum_pipeline = Pipeline([\n  ('imputer', SimpleImputer(strategy='median'))\n])\n\ncat_pipeline = Pipeline([\n  ('imputer', SimpleImputer(strategy='most_frequent')),\n])\n\n# modified the previous `full_pipeline` to include the `num_pipeline` and `cat_pipeline`\nfull_pipeline = ColumnTransformer([\n  ('drop_columns', 'drop', drop_columns),\n  ('num', num_pipeline, num_features),\n  ('cat', cat_pipeline, cat_features)\n])\n")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Detour - not really")," "),(0,l.kt)("p",null,"If we use the same cell to print out the result of the new transformation, we'll notice the column names are not in the correct order anymore."),(0,l.kt)("p",null,"Check out the ",(0,l.kt)("inlineCode",{parentName:"p"},"fare")," column for example to realize what's happened."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Apply the Transformation\ntransformed_train_set = full_pipeline.fit_transform(titanic_X)\n\n# Transform the numpy n-dimensional array into a pandas dataframe\ntransformed_train_set = pd.DataFrame(transformed_train_set, columns=titanic_X.columns.drop(drop_columns), index=titanic_X.index)\ntransformed_train_set.head()\n")),(0,l.kt)(m,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>801</th>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.75</td>\n      <td>3</td>\n      <td>male</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>22.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>29.0</td>\n      <td>2</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>48.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>65.0</td>\n      <td>2</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10.5</td>\n      <td>2</td>\n      <td>male</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>41.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>20.2125</td>\n      <td>3</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("p",null,"The pipeline reorders the features in the order provided in the ",(0,l.kt)("inlineCode",{parentName:"p"},"ColumnTransformer")," for the ",(0,l.kt)("inlineCode",{parentName:"p"},"num_pipeline")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"cat_pipeline")),(0,l.kt)("p",null,"to get the list of the features in the new order, we can use the ",(0,l.kt)("inlineCode",{parentName:"p"},"get_feature_names_out()")," method on the ",(0,l.kt)("inlineCode",{parentName:"p"},"full_pipeline")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"full_pipeline.get_feature_names_out()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    array(['num__age', 'num__sibsp', 'num__parch', 'num__fare', 'cat__pclass',\n           'cat__sex', 'cat__embarked'], dtype=object)\n"))),(0,l.kt)("p",null,"Note how the pipeline prefixes the feature names with the name of the pipeline"),(0,l.kt)("p",null,"To get a list of the original features names  in the new order, I will use a technique called ",(0,l.kt)("strong",{parentName:"p"},"List Comprehension")," to construct a new list based on looping over the new names and removing the prefix with the ",(0,l.kt)("inlineCode",{parentName:"p"},"replace")," function."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"column_names = [ \n  feature.replace('num__', '').replace('cat__', '') \n  for feature in full_pipeline.get_feature_names_out()\n]\ncolumn_names\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    ['age', 'sibsp', 'parch', 'fare', 'pclass', 'sex', 'embarked']\n"))),(0,l.kt)("p",null,"Let's run the transformation and conversion cell again, and see the result."),(0,l.kt)("p",null,"Here we'll update the columns property provided to the ",(0,l.kt)("inlineCode",{parentName:"p"},"DataFrame")," constructor to use the new list of feature names."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Apply the Transformation\ntransformed_train_set = full_pipeline.fit_transform(titanic_X)\n\n# Transform the numpy n-dimensional array into a pandas dataframe\ntransformed_train_set = pd.DataFrame(transformed_train_set, columns=column_names, index=titanic_X.index)\ntransformed_train_set.head()\n")),(0,l.kt)(m,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>801</th>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.75</td>\n      <td>3</td>\n      <td>male</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>22.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>29.0</td>\n      <td>2</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>48.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>65.0</td>\n      <td>2</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10.5</td>\n      <td>2</td>\n      <td>male</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>41.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>20.2125</td>\n      <td>3</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("p",null,"to prove that no missing values exist anymore:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"transformed_train_set.isnull().sum()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    age         0\n    sibsp       0\n    parch       0\n    fare        0\n    pclass      0\n    sex         0\n    embarked    0\n    dtype: int64\n"))),(0,l.kt)("h3",{id:"3-standardization-and-normalization"},"3. Standardization and Normalization"),(0,l.kt)("h4",{id:"the-why-1"},"The Why..."),(0,l.kt)("p",null,"We need to standardize the numerical features, because we want to make sure that the features are on the same scale. If we don't do this, the features with a higher scale will have a higher weight in the model."),(0,l.kt)("p",null,"Not all models require this step, decision trees for example, don't require this step. But we're going to use other models as well, so we'll do this for good measure."),(0,l.kt)("h4",{id:"the-how-1"},"The How..."),(0,l.kt)("p",null,"We can use the ",(0,l.kt)("inlineCode",{parentName:"p"},"StandardScaler")," to standardize the numerical features. ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"},"\ud83d\udcdc Learn more about this here")),(0,l.kt)("p",null,"This is only applied to numerical features. It can be applied to categorical features as well, but it's not recommended, categorical features don't necessarily follow a normal distribution."),(0,l.kt)("p",null,"We will need to modify the pipeline to add this step to numerical pipeline."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"num_features = ['age', 'sibsp', 'parch', 'fare']\ncat_features = ['pclass', 'sex', 'embarked']\n\nnum_pipeline = Pipeline([\n  ('imputer', SimpleImputer(strategy='median')),\n  ('scale', StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n  ('imputer', SimpleImputer(strategy='most_frequent')),\n])\n\n# modified the previous `full_pipeline` to include the `num_pipeline` and `cat_pipeline`\nfull_pipeline = ColumnTransformer([\n  ('drop_columns', 'drop', drop_columns),\n  ('num', num_pipeline, num_features),\n  ('cat', cat_pipeline, cat_features)\n])\n")),(0,l.kt)("h3",{id:"4-handling-categorical-features"},"4. Handling Categorical Features"),(0,l.kt)("p",null,"As we've mentioned before, we deal with categorical features in 2 ways:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Ordinal Encoding - for Ordinal features - features that have a natural order, or ranking. (like ",(0,l.kt)("inlineCode",{parentName:"li"},"pclass")," for example)"),(0,l.kt)("li",{parentName:"ol"},"One-Hot Encoding - for Nominal features - features that don't have a particular order, or ranking. (like ",(0,l.kt)("inlineCode",{parentName:"li"},"sex")," for example)")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("strong",{parentName:"p"},"Interesting Thought \ud83e\udde0\ud83e\uddd0"),": Could the point of passenger embarkation have an effect on their chances of survival? How you answer this question will determine how you encode this feature.\nPassengers boarded the Titanic in 3 different ports: Southampton, England,  then Cherbourg, France, and Finally Queenstown, Ireland."),(0,l.kt)("ul",{parentName:"blockquote"},(0,l.kt)("li",{parentName:"ul"},"If you think that the point of embarkation has no effect on the chances of survival, then this piece of information is a nominal data, then you can use One-Hot Encoding."),(0,l.kt)("li",{parentName:"ul"},"If you think that having boarded the Titanic earlier, or later, has an effect on the chances of survival, For example, having boarded early, passengers may have gotten better at getting around the ship and know faster paths to the lifeboats, then this piece of information is an ordinal data, then you can use Ordinal Encoding."),(0,l.kt)("li",{parentName:"ul"},"If you think that passengers at different points of embarkation were given different information, and given better training at the different ports (so it's not the order that matter but rather the location itself), then this would be a nominal data, then you can use One-Hot Encoding."))),(0,l.kt)("p",null,"We will need to modify the pipeline to handle the different categorical data differently."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"num_features = ['age', 'sibsp', 'parch', 'fare']\ncat_features = ['pclass', 'sex', 'embarked']\nordinal_cat_features = ['pclass']\nnominal_cat_features = [ 'sex', 'embarked']\n\nnum_pipeline = Pipeline([\n  ('imputer', SimpleImputer(strategy='median')),\n  ('scale', StandardScaler())\n])\n\nordinal_cat_pipeline = Pipeline([\n  ('imputer', SimpleImputer(strategy='most_frequent')),\n  ('encode', OrdinalEncoder())\n])\n\nnominal_cat_pipeline = Pipeline([\n  ('imputer', SimpleImputer(strategy='most_frequent')),\n  ('encode', OneHotEncoder())\n])\n\n# modified the previous `full_pipeline` to include the `num_pipeline` and `cat_pipeline`\nfull_pipeline = ColumnTransformer([\n  ('drop_columns', 'drop', drop_columns),\n  ('num', num_pipeline, num_features),\n  ('ordinal_cat', ordinal_cat_pipeline, ordinal_cat_features),\n  ('nominal_cat', nominal_cat_pipeline, nominal_cat_features)\n])\n")),(0,l.kt)("p",null,"Let's apply the transformation and conversion cell again, and see the result."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Apply the Transformation\ntransformed_train_set = full_pipeline.fit_transform(titanic_X)\n\n# Get the new column names after the new transformations\ncolumn_names = [ \n  feature\n    .replace('num__', '')\n    .replace('cat__', '') \n    .replace('ordinal_', '') \n    .replace('nominal_', '') \n  for feature in full_pipeline.get_feature_names_out()\n]\ndisplay(column_names)\n\n# Transform the numpy n-dimensional array into a pandas dataframe\ntransformed_train_set = pd.DataFrame(transformed_train_set, columns=column_names, index=titanic_X.index)\ntransformed_train_set.head()\n")),(0,l.kt)(m,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"['age',\n 'sibsp',\n 'parch',\n 'fare',\n 'pclass',\n 'sex_female',\n 'sex_male',\n 'embarked_C',\n 'embarked_Q',\n 'embarked_S']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>pclass</th>\n      <th>sex_female</th>\n      <th>sex_male</th>\n      <th>embarked_C</th>\n      <th>embarked_Q</th>\n      <th>embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>801</th>\n      <td>-0.106890</td>\n      <td>-0.469588</td>\n      <td>-0.441078</td>\n      <td>-0.486942</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>-0.578548</td>\n      <td>0.481396</td>\n      <td>0.728058</td>\n      <td>-0.063742</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>1.465301</td>\n      <td>0.481396</td>\n      <td>1.897193</td>\n      <td>0.653210</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>-0.578548</td>\n      <td>-0.469588</td>\n      <td>-0.441078</td>\n      <td>-0.432175</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>0.915034</td>\n      <td>-0.469588</td>\n      <td>1.897193</td>\n      <td>-0.238748</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("p",null,"One thing we should note is that the ",(0,l.kt)("inlineCode",{parentName:"p"},"sex_female")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"sex_male")," columns are mutually exclusive. only one of them can be ",(0,l.kt)("inlineCode",{parentName:"p"},"1")," at a time. This is called a ",(0,l.kt)("strong",{parentName:"p"},"Dummy Variable Trap"),". We can drop one of the columns to avoid this trap."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"clean_titanic_X = transformed_train_set.drop('sex_female', axis=1).rename(columns={ 'sex_male': 'is_male' })\nclean_titanic_X.head()\n")),(0,l.kt)(m,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>pclass</th>\n      <th>is_male</th>\n      <th>embarked_C</th>\n      <th>embarked_Q</th>\n      <th>embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>801</th>\n      <td>-0.106890</td>\n      <td>-0.469588</td>\n      <td>-0.441078</td>\n      <td>-0.486942</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>-0.578548</td>\n      <td>0.481396</td>\n      <td>0.728058</td>\n      <td>-0.063742</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>1.465301</td>\n      <td>0.481396</td>\n      <td>1.897193</td>\n      <td>0.653210</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>-0.578548</td>\n      <td>-0.469588</td>\n      <td>-0.441078</td>\n      <td>-0.432175</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>0.915034</td>\n      <td>-0.469588</td>\n      <td>1.897193</td>\n      <td>-0.238748</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"I couldn't do the above un the pipeline. Not sure if it's possible.")),(0,l.kt)("p",null,"at this point, we have a clean dataset that we can use to train our model. Remember, the machine learning model development process is an iterative one. We can also come back to this step and add more transformations. For example, we can add a new transformation to handle outliers, we can introduce new features, or we can drop features that we think are not useful."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"display(clean_titanic_X.shape)\ndisplay(titanic_y.shape)\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    (1047, 9)\n\n\n\n    (1047,)\n"))),(0,l.kt)("hr",null),(0,l.kt)("h2",{id:"4\ufe0f\u20e3-analyze"},"4\ufe0f\u20e3. Analyze"),(0,l.kt)("p",null,"At this step, we'll work on developing multiple models, and then select the best one. We'll use the ",(0,l.kt)("inlineCode",{parentName:"p"},"clean_titanic_X")," ro train the model, and do some initial evaluation of the model accuracy and performance. Once we're satisfied with the results we're getting, we'll the test set to evaluate the model and ensure that's it's not overfitting, or underfitting."),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"Model Evaluation",src:n(66626).Z,width:"1200",height:"684"})),(0,l.kt)("p",null,"One Approach I've seen in classification problems, is to use a ",(0,l.kt)("em",{parentName:"p"},"Dummy Classifier")," as a baseline. This is a classifier that makes predictions without considering the input. The predictions can be random, based on the most frequent class in the dataset, ... etc."),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"},"\ud83d\udcdc Learn more: Dummy Classifier")),(0,l.kt)("h3",{id:"dummy-classifier"},"Dummy Classifier"),(0,l.kt)("p",null,"This is a good baseline to compare our model to. If our model is better than the dummy classifier, then we know that we're on the right track."),(0,l.kt)("p",null,"Let's start with that."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# imported DummyClassifier and the metrics packages (first cell)\ndummy_classifier = DummyClassifier(strategy='stratified')\ndummy_classifier.fit(clean_titanic_X, titanic_y)\ndummy_classifier.score(clean_titanic_X, titanic_y) # accuracy\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    0.5071633237822349\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"dummy_classifier.predict(clean_titanic_X)\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    array([0, 1, 0, ..., 1, 0, 0])\n"))),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Try different strategies for the dummy classifier, and see how it affects the results.")),(0,l.kt)("p",null,"Remember in previous modules, we talked about using the cross-validation technique to evaluate the model. We'll use the same technique here."),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/cross_validation.html"},"\ud83d\udcdc Learn more: SkLearn-cross validation")),(0,l.kt)("p",null,"Similar to how we used the ",(0,l.kt)("inlineCode",{parentName:"p"},"train_test_split")," function to split the data into training and test sets, we'll use the ",(0,l.kt)("inlineCode",{parentName:"p"},"cross_val_score")," function to split the training data into training and validation sets, and then evaluate the model on each of the validation sets."),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"},"\ud83d\udcdc Learn more: ",(0,l.kt)("inlineCode",{parentName:"a"},"cross_val_score")," function")),(0,l.kt)("p",null,"Let's apply that to our dummy classifier. "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'scores = cross_val_score(\n  dummy_classifier, clean_titanic_X, titanic_y,\n  scoring="roc_auc", cv=10)\nprint(\n    f"Dummy Classifier  AUC: {scores.mean():.3f} STD: {scores.std():.2f}"\n)\n')),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Dummy Classifier  AUC: 0.495 STD: 0.04\n"))),(0,l.kt)("p",null,"Similar to when we learned about linear regression, I've mentioned that we use the mean squared errors to calculate the RMSE, the model's effectiveness, in classification problems, we use the (Area Under Curve) AUC to calculate the ROC AUC."),(0,l.kt)("p",null,"AUC is a good metric to use for classification problems. It's a measure of how well the model can distinguish between the positive and negative classes. The higher the AUC, the better the model is at distinguishing between the positive and negative classes."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"},"\ud83d\udcdc Learn more about this here: Understanding AUC - ROC Curve")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"},"\ud83d\udcdc Documentation"))),(0,l.kt)("h3",{id:"logistic-classifier"},"Logistic Classifier"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"logistic_classifier = LogisticRegression()\nlogistic_classifier.fit(clean_titanic_X, titanic_y)\n")),(0,l.kt)(m,{center:!0,mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "\u25b8";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "\u25be";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter\'s `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression()</pre></div></div></div></div></div>\n'))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'scores = cross_val_score(\n  logistic_classifier, clean_titanic_X, titanic_y,\n  scoring="roc_auc", cv=10)\nprint(\n    f"Logistic Classifier  AUC: {scores.mean():.3f} STD: {scores.std():.2f}"\n)\n')),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Logistic Classifier  AUC: 0.848 STD: 0.05\n"))),(0,l.kt)("p",null,"For the ",(0,l.kt)("inlineCode",{parentName:"p"},"cross_val_score")," funciton, the scoring is selected such that the higher the score, the better the model is. So, we'll use the ",(0,l.kt)("inlineCode",{parentName:"p"},"roc_auc_score")," function to calculate the AUC score."),(0,l.kt)("p",null,"From the above we see that logisitic regression is better than the dummy classifier. (higher AUC value)."),(0,l.kt)("p",null,"Now let's try to be a bit more efficient, and test out multiple models at once. The process is the same for all models, we fit, ane use the ",(0,l.kt)("inlineCode",{parentName:"p"},"cross_val_score")," function to evaluate the model."),(0,l.kt)("p",null,"We'll test the following: "),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"DummyClassifier,"),(0,l.kt)("li",{parentName:"ul"},"LogisticRegression,"),(0,l.kt)("li",{parentName:"ul"},"DecisionTreeClassifier,"),(0,l.kt)("li",{parentName:"ul"},"KNeighborsClassifier,"),(0,l.kt)("li",{parentName:"ul"},"GaussianNB,"),(0,l.kt)("li",{parentName:"ul"},"SVC,"),(0,l.kt)("li",{parentName:"ul"},"RandomForestClassifier,"),(0,l.kt)("li",{parentName:"ul"},"xgboost.XGBClassifier,")),(0,l.kt)("p",null,"The following will run the models, without any hyperparameter tuning (default/out-of-the-box configurations)."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'for model in [\n    DummyClassifier,\n    LogisticRegression,\n    DecisionTreeClassifier,\n    KNeighborsClassifier,\n    GaussianNB,\n    SVC,\n    RandomForestClassifier,\n]:\n    classifier_model = model()\n    # defining the kfolds, will ensure that all models will be trained with the same data\n    kfold = KFold(\n        n_splits=10, random_state=42, shuffle=True\n    )\n    scores = cross_val_score(\n        classifier_model,\n        clean_titanic_X, \n        titanic_y, \n        scoring="roc_auc", cv=kfold\n    )\n    print(\n    f"{model.__name__:22}  AUC: {scores.mean():.3f} STD: {scores.std():.2f}"\n)\n')),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    DummyClassifier         AUC: 0.500 STD: 0.00\n    LogisticRegression      AUC: 0.849 STD: 0.03\n    DecisionTreeClassifier  AUC: 0.750 STD: 0.04\n    KNeighborsClassifier    AUC: 0.823 STD: 0.04\n    GaussianNB              AUC: 0.814 STD: 0.04\n    SVC                     AUC: 0.845 STD: 0.03\n    RandomForestClassifier  AUC: 0.846 STD: 0.04\n"))),(0,l.kt)("p",null,"Now we've mentioned that a good model would be the one that has a high AUC score, but here I also want to add that we may go with an algorithm that has a slightly smaller score but tighter standard deviation. This means that the model is more consistent, and less likely to overfit."),(0,l.kt)("p",null,"from the results above, looks like the Logistic Regression, SVC and RandomForestClassifier are the best models. Let's evaluate them using the test set."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Remember"),": our baseline for choosing the best model was the dummy classifier, and the AUC scores."),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("strong",{parentName:"p"},"\ud83e\udd89 Note"),": Understanding the different hyperparameters of each model is important. However it's well-outside the scope of this course. However, you can always start with the sklearn documentation for each model to get an idea of the different hyperparameters and their usages.")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# Bringing back the logistic classifier\nlogistic_classifier = LogisticRegression()\nlogistic_classifier.fit(clean_titanic_X, titanic_y)\n\nscores = cross_val_score(\n  logistic_classifier, clean_titanic_X, titanic_y,\n  scoring="roc_auc", cv=10)\nprint(\n    f"Logistic Classifier  AUC: {scores.mean():.3f} STD: {scores.std():.2f}"\n)\n')),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Logistic Classifier  AUC: 0.848 STD: 0.05\n"))),(0,l.kt)("p",null,"In addition to the AUC score, there are other metrics that we can use to evaluate a model."),(0,l.kt)("p",null,"The following resource explains most of those metrics."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/understanding-classification-metrics-in-scikit-learn-in-python-3bc336865019"},"\ud83d\udcdc Learn more about this here: Understanding Classification Metrics in Sklearn"))),(0,l.kt)("p",null,"I will show:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},".score")," method,",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"this returns the mean accuracy of the given X, and y.(prediction accuracy)."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"precision_score"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"this returns the ration of the true positives to the sum of the true positives and false positives. ",(0,l.kt)("span",{parentName:"li",className:"math math-inline"},(0,l.kt)("span",{parentName:"span",className:"katex"},(0,l.kt)("span",{parentName:"span",className:"katex-mathml"},(0,l.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,l.kt)("semantics",{parentName:"math"},(0,l.kt)("mrow",{parentName:"semantics"},(0,l.kt)("mfrac",{parentName:"mrow"},(0,l.kt)("mrow",{parentName:"mfrac"},(0,l.kt)("mi",{parentName:"mrow"},"t"),(0,l.kt)("mi",{parentName:"mrow"},"p")),(0,l.kt)("mrow",{parentName:"mfrac"},(0,l.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,l.kt)("mi",{parentName:"mrow"},"t"),(0,l.kt)("mi",{parentName:"mrow"},"p"),(0,l.kt)("mo",{parentName:"mrow"},"+"),(0,l.kt)("mi",{parentName:"mrow"},"f"),(0,l.kt)("mi",{parentName:"mrow"},"p"),(0,l.kt)("mo",{parentName:"mrow",stretchy:"false"},")")))),(0,l.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\frac{tp}{(tp+fp)}")))),(0,l.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,l.kt)("span",{parentName:"span",className:"base"},(0,l.kt)("span",{parentName:"span",className:"strut",style:{height:"1.3967em",verticalAlign:"-0.52em"}}),(0,l.kt)("span",{parentName:"span",className:"mord"},(0,l.kt)("span",{parentName:"span",className:"mopen nulldelimiter"}),(0,l.kt)("span",{parentName:"span",className:"mfrac"},(0,l.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,l.kt)("span",{parentName:"span",className:"vlist-r"},(0,l.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.8767em"}},(0,l.kt)("span",{parentName:"span",style:{top:"-2.655em"}},(0,l.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,l.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,l.kt)("span",{parentName:"span",className:"mord mtight"},(0,l.kt)("span",{parentName:"span",className:"mopen mtight"},"("),(0,l.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"tp"),(0,l.kt)("span",{parentName:"span",className:"mbin mtight"},"+"),(0,l.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.10764em"}},"f"),(0,l.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"p"),(0,l.kt)("span",{parentName:"span",className:"mclose mtight"},")")))),(0,l.kt)("span",{parentName:"span",style:{top:"-3.23em"}},(0,l.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,l.kt)("span",{parentName:"span",className:"frac-line",style:{borderBottomWidth:"0.04em"}})),(0,l.kt)("span",{parentName:"span",style:{top:"-3.4461em"}},(0,l.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,l.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,l.kt)("span",{parentName:"span",className:"mord mtight"},(0,l.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"tp"))))),(0,l.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,l.kt)("span",{parentName:"span",className:"vlist-r"},(0,l.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.52em"}},(0,l.kt)("span",{parentName:"span"}))))),(0,l.kt)("span",{parentName:"span",className:"mclose nulldelimiter"}))))))))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"confusion_matrix"),",",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"this returns the confusion matrix which is a table that shows the number of true positives, true negatives, false positives, and false negatives.")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# `.score` on training data\nlogistic_classifier.score(clean_titanic_X, titanic_y)\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    0.7965616045845272\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# `precision_score` on training data\nmetrics.precision_score(titanic_y, logistic_classifier.predict(clean_titanic_X))\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    0.7493333333333333\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Confusion Matrix\nmetrics.confusion_matrix(titanic_y, logistic_classifier.predict(clean_titanic_X))\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    array([[553,  94],\n           [119, 281]])\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# plotting the confusion matrix\n# the metrics packages has a `plot_confusion_matrix` function\nmetrics.ConfusionMatrixDisplay.from_estimator(\n  estimator=logistic_classifier,\n  X=clean_titanic_X, y=titanic_y,\n  display_labels=['Died', 'Survived'],\n  cmap=\"Blues\", colorbar=False\n)\nplt.show()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(27861).Z,width:"491",height:"436"}))),(0,l.kt)("p",null,"We can attribute the inaccuracy of the model to either that model is not a good fit for the data (underfitting), or that we don't have enough data to train the model."),(0,l.kt)("p",null,"For this particular problem, those scores can be considered good enough. However, we can still improve the model by tuning the hyperparameters."),(0,l.kt)("p",null,"Looking at the documentation for the ",(0,l.kt)("inlineCode",{parentName:"p"},"LogisticRegression")," model, we can see that there are a lot of hyperparameters that we can tune. ",(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score"},"\ud83d\udcdc LogisticRegression"),". finding the best combination of the matrix that would get us the best outcome can be a very tedious process. especially with this number of parameters. ",(0,l.kt)("em",{parentName:"p"},"However,")," We can use the ",(0,l.kt)("inlineCode",{parentName:"p"},"GridSearchCV")," function to find the best combination of hyperparameters."),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"},"\ud83d\udcdc Learn more about this here")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# first we define the different options for each of the hyperparameters\nparam_grid = {\n  'penalty': ['l2'],\n  'C': [ 100, 10, 1, 0.1, 0.01, 0.001],\n  'solver': ['liblinear', 'lbfgs', 'newton-cg']\n}\n\n# then we define the grid search\ngrid_search = GridSearchCV(\n  estimator=LogisticRegression(),\n  param_grid=param_grid,\n  n_jobs=-1,\n  scoring='roc_auc',\n).fit(clean_titanic_X, titanic_y)\nprint(f\"Best Score: {grid_search.best_score_}\")\nprint(f\"Best Params: {grid_search.best_params_}\")\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Best Score: 0.8477857036374479\n    Best Params: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n"))),(0,l.kt)("p",null,"Looks like we were able to find a better combination of hyperparameters."),(0,l.kt)("p",null,"if we're curious to see the results of the other combinations, we can do the following:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"means = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nparams = grid_search.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    0.847426 (0.025140) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n    0.847464 (0.025137) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n    0.847464 (0.025137) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n    0.847387 (0.025229) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n    0.847368 (0.025135) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n    0.847368 (0.025135) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n    0.847330 (0.025299) with: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n    0.847175 (0.025162) with: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n    0.847175 (0.025162) with: {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n    0.847786 (0.025439) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n    0.846199 (0.025769) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n    0.846199 (0.025769) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n    0.832383 (0.029007) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n    0.831354 (0.028107) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n    0.831354 (0.028107) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n    0.793599 (0.030320) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n    0.797472 (0.032472) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n    0.797472 (0.032472) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'newton-cg'}\n"))),(0,l.kt)("p",null,"Let's generate the confusion matrix for this new model"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# First create the new model with the best parameters\nupdated_logistic_classifier = LogisticRegression(\n  C=0.1, penalty='l2', solver='liblinear'\n)\n# Then fit the model\nupdated_logistic_classifier.fit(clean_titanic_X, titanic_y)\n\n# Then plot the confusion matrix\nmetrics.ConfusionMatrixDisplay.from_estimator(\n  estimator=updated_logistic_classifier,\n  X=clean_titanic_X, y=titanic_y,\n  display_labels=['Died', 'Survived'],\n  cmap=\"Blues\", colorbar=False\n)\nplt.show()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(50253).Z,width:"491",height:"436"}))),(0,l.kt)("p",null," Let's evaluate the model using the test set."),(0,l.kt)("p",null," ",(0,l.kt)("strong",{parentName:"p"},"Remember"),", now that we have a model, our baseline for evaluating the model are the metrics on training set, compared to the metrics on the test set."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# EVALUATION ON TEST SET\n# First we need to transform the test set\ntitanic_test_X = test_set.drop('survived', axis=1)\ntitanic_test_y = test_set['survived'].copy()\n\n# Be Careful, we shouldn't call `fit` or `fit_transform` on the test set\ntransformed_test_set = full_pipeline.transform(titanic_test_X)\ntransformed_test_set = pd.DataFrame(transformed_test_set, columns=column_names, index=titanic_test_X.index)\nclean_titanic_test_X = transformed_test_set.drop('sex_female', axis=1).rename(columns={ 'sex_male': 'is_male' })\n\nclean_titanic_test_X.head()\n")),(0,l.kt)(m,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>pclass</th>\n      <th>is_male</th>\n      <th>embarked_C</th>\n      <th>embarked_Q</th>\n      <th>embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>545</th>\n      <td>0.050329</td>\n      <td>2.383366</td>\n      <td>-0.441078</td>\n      <td>-0.223064</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>0.443377</td>\n      <td>0.481396</td>\n      <td>-0.441078</td>\n      <td>1.151093</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-0.421329</td>\n      <td>-0.469588</td>\n      <td>0.728058</td>\n      <td>4.288171</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>0.050329</td>\n      <td>-0.469588</td>\n      <td>-0.441078</td>\n      <td>-0.088636</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1094</th>\n      <td>0.128938</td>\n      <td>-0.469588</td>\n      <td>-0.441078</td>\n      <td>-0.468355</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# `.score` on test data\nupdated_logistic_classifier.score(clean_titanic_test_X, titanic_test_y)\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    0.7786259541984732\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# # `precision_score` on test data\nmetrics.precision_score(titanic_test_y, updated_logistic_classifier.predict(clean_titanic_test_X))\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    0.7560975609756098\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Then we can plot the confusion matrix\nmetrics.ConfusionMatrixDisplay.from_estimator(\n  estimator=updated_logistic_classifier,\n  X=clean_titanic_test_X, y=titanic_test_y,\n  display_labels=['Died', 'Survived'],\n  cmap=\"Blues\", colorbar=False\n)\nplt.show()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(31905).Z,width:"491",height:"436"}))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# And of Course, if we needed to make a prediction on new data points, we can do it like this\n# here we're making predictions about 3 records at the same time\nnew_data = pd.DataFrame({\n  'pclass': [1, 2, 3],\n  'name': ['John', 'Jane', 'Joe'], # doesn't matter\n  'sex': ['male', 'female', 'male'],\n  'age': [30, 25, 20],\n  'sibsp': [0, 0, 0],\n  'parch': [0, 0, 0],\n  'ticket': ['123', '456', '789'], # doesn't matter\n  'fare': [100, 50, 25],\n  'cabin': ['A', 'B', 'C'],\n  'embarked': ['S', 'C', 'Q'],\n  'boat': ['A', 'B', 'C'], # doesn't matter\n  'body': [1, 2, 3], # doesn't matter\n  'home.dest': ['London', 'Paris', 'New York'], # doesn't matter\n})\n# First we need to transform the new data\ntransformed_new_data = full_pipeline.transform(new_data)\ntransformed_new_data = pd.DataFrame(transformed_new_data, columns=column_names, index=new_data.index)\nclean_new_data = transformed_new_data.drop('sex_female', axis=1).rename(columns={ 'sex_male': 'is_male' })\n\n\nupdated_logistic_classifier.predict(clean_new_data)\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    array([0, 1, 0])\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Also if we're making a predictions for only one record, we can do it like this\nnew_data2 = pd.DataFrame.from_records([{\n  'pclass': 1,\n  'name': 'John', # doesn't matter\n  'sex': 'male',\n  'age': 25,\n  'sibsp': 0,\n  'parch': 0,\n  'ticket': '123', # doesn't matter\n  'fare': 100,\n  'cabin': 'A',\n  'embarked': 'C',\n  'boat': 'A', # doesn't matter; leaked data; will be dropped\n  'body': 1, # doesn't matter; leaky data ; will be dropped\n  'home.dest': 'London', # doesn't matter - will be removed\n}])\n\n# transform the new data\ntransformed_new_data2 = full_pipeline.transform(new_data2)\ntransformed_new_data2 = pd.DataFrame(transformed_new_data2, columns=column_names, index=new_data2.index)\nclean_new_data2 = transformed_new_data2.drop('sex_female', axis=1).rename(columns={ 'sex_male': 'is_male' })\n\n\nupdated_logistic_classifier.predict(clean_new_data2)\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    array([1])\n"))),(0,l.kt)("p",null,"This concludes the classification exercise for one algorithm. In the assignment, you'll be asked to do the same with another dataset."),(0,l.kt)("p",null,"I also encourage you to try the fine-tuning work on this dataset using different algorithms and see how it affects the results. (5 points of extra credit)"),(0,l.kt)("hr",null),(0,l.kt)("h2",{id:"5\ufe0f\u20e3-deploy"},"5\ufe0f\u20e3. Deploy"),(0,l.kt)("p",null,"The last thing I want to mention is regarding the deployment of the models developed."),(0,l.kt)("p",null,"Creating web applications or software solutions to house the model would be outside the scope of this course, but that's not the only aspect of deployment."),(0,l.kt)("p",null,"You may have wondered how do you deploy the model when it's so dependant on the data, and what do you do when the data is millions of records. The answer is that you don't. You deploy the model separately."),(0,l.kt)("p",null,"The model can be deployed in a variety of ways, and the most common way is to use the pickle library to save the model, and then load it in the application."),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/pickle.html"},"\ud83d\udcdc Learn more about this here: Pickle")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Deploying the model\npic = pickle.dump(updated_logistic_classifier, file=open('model.pkl', 'wb'))\n")),(0,l.kt)("p",null,"At any time in the future, if you want to use the model, you can load it, and use it to make predictions."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"#load the model \nloaded_model = pickle.load(open('model.pkl', 'rb'))\n\nloaded_model.predict(clean_new_data2)\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    array([1])\n"))))}u.isMDXComponent=!0},97020:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_11_0-8ab839b645bcee66a02b56edd2e5fb0e.png"},27861:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_74_0-e72f64866c15da132b48f6382ad33e80.png"},50253:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_80_0-75f52719f228180ab9923befd02b0b3c.png"},31905:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_85_1-e791522fb673edb9e7048aa615ac35c6.png"},66626:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/evaluate-model-f483636ff1954c354fe430bceac49519.webp"},50526:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/simple-classification-319415bc780c9ab2c3eed93eff4ff1fc.png"}}]);