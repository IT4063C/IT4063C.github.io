"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[4733],{17901:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>i});var a=n(87462),d=(n(67294),n(3905));n(8209);const o={title:"Exploratory Data Analysis (EDA)",sidebar_position:1,draft:!1},l=void 0,s={unversionedId:"eda/index",id:"eda/index",title:"Exploratory Data Analysis (EDA)",description:"What is EDA?",source:"@site/content/course-notes/6.eda/index.md",sourceDirName:"6.eda",slug:"/eda/",permalink:"/course-notes/eda/",draft:!1,editUrl:"https://github.com/IT4063C/IT4063C.github.io/tree/main/content/course-notes/6.eda/index.md",tags:[],version:"current",lastUpdatedBy:"Yahya Gilany",lastUpdatedAt:1665589749,formattedLastUpdatedAt:"Oct 12, 2022",sidebarPosition:1,frontMatter:{title:"Exploratory Data Analysis (EDA)",sidebar_position:1,draft:!1},sidebar:"notes",previous:{title:"Importing from Different Data Sources",permalink:"/course-notes/working-with-data/data-sources"},next:{title:"Visualization",permalink:"/course-notes/visualizations/"}},r={},i=[{value:"What is EDA?",id:"what-is-eda",level:2},{value:"Tidy Data and Data structures",id:"tidy-data-and-data-structures",level:2},{value:"Data Types",id:"data-types",level:2},{value:"Why do we care?",id:"why-do-we-care",level:3},{value:"Data Cleaning",id:"data-cleaning",level:2},{value:"Duplicate Data",id:"duplicate-data",level:3},{value:"Missing Data, Anamolies, and Outliers",id:"missing-data-anamolies-and-outliers",level:3},{value:"About Statistical Analysis",id:"about-statistical-analysis",level:2},{value:"Data Visualization",id:"data-visualization",level:2},{value:"Data Correlations",id:"data-correlations",level:2},{value:"Assignments",id:"assignments",level:4},{value:"References",id:"references",level:2}],u=t=>function(e){return console.warn("Component "+t+" was not imported, exported, or provided by MDXProvider as global scope"),(0,d.kt)("div",e)},h=u("HTMLOutputBlock"),p=u("CodeOutputBlock"),m={toc:i};function c(t){let{components:e,...o}=t;return(0,d.kt)("wrapper",(0,a.Z)({},m,o,{components:e,mdxType:"MDXLayout"}),(0,d.kt)("h2",{id:"what-is-eda"},"What is EDA?"),(0,d.kt)("p",null,"Classical Statistical analysis was almost always focusing on what's known inferential statistics by deriving estimates about a population or by by hypotheses testing or confirmatory data analysis."),(0,d.kt)("p",null,"Confirmatory Data analysis is For example, when you want to know if a new drug is effective in treating a disease. Your hypothesis could be that the drug is effective, and you want to use data to prove that. so you design an experiment where you give the drug to a group of patients and you give a placebo to another group of patients. You then measure the outcome of the disease in both groups and compare the results. If the results show that that to a certain degree of confidence (p-value) the drug is effective, you can conclude that your hypothesis was correct. This is an example of confirmatory data analysis."),(0,d.kt)("p",null,"estimating parameters about a population is for example when you're looking to see how many of the university community like a certain service at the university. You can't ask everyone. I mean it's difficult enough to get students to submit assignments on time. So you take a sample of students and ask them. You then use the sample to estimate the number of students who like the service. This is an example of estimating parameters about a population."),(0,d.kt)("p",null,"so in 1961, John Tukey, a prominent mathematician and a statistician, coined the term Exploratory Data Analysis (EDA). That is a process of analyzing data sets for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. a process, where the hypothesis can be suggested by the data itself, not just using the data to confirm or reject a hypothesis suggested by the researcher."),(0,d.kt)("p",null,"The basic ",(0,d.kt)("strong",{parentName:"p"},"goals")," or EDA are:"),(0,d.kt)("ol",null,(0,d.kt)("li",{parentName:"ol"},"remove or correct erroneous data - data cleaning "),(0,d.kt)("li",{parentName:"ol"},"formulate initial hypotheses and draw on a few insights based on the initial investigations of the data."),(0,d.kt)("li",{parentName:"ol"},"choose suitable analysis methods")),(0,d.kt)("p",null,"For that we need to\nLOOK AT THE DATA - SEE THE DATA"),(0,d.kt)("p",null,"For that we need:"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Statistics"),(0,d.kt)("li",{parentName:"ul"},"Data Cleaning techniques"),(0,d.kt)("li",{parentName:"ul"},"Data Visualizations")),(0,d.kt)("p",null,"They'll all go hand in hand. Statistics will help you understand some insights about the data. That understanding and those insights will help us in data cleaning and I will show a few examples there. Also just looking at the Raw data may cause us to miss huge opportunities and insights in the data. The Raw data and the statistics by them selves can deceiving sometimes.\nAnd Visualizing the data will help you in coming up with your initial hypothesis about the data and determine the appropriate analysis methods and models. Sometimes even, visualization will help you in data cleaning and data understanding as you'll see statistics can be deceiving sometimes."),(0,d.kt)("p",null,(0,d.kt)("strong",{parentName:"p"},"This is an iterative process"),"\nRemember, the data analysis process is not linear, it is an iterative, incremental process;\nwe don't collect data, then clean the data, then analyze the data.\nInstead this all happens in iterations; we collect the data, and do some initial analysis, based on which we clean the data, then do some more analysis, then maybe we then realize we don't have enough data for the analysis so we go back and collect some more and so on and so forth."),(0,d.kt)("p",null,"I say this because I want to emphasize that EDA is not a one-time thing.\nand because many of the topics we will cover in this module are topics that we'll keep coming back to again and again throughout the course, with more information. "),(0,d.kt)("p",null,"But before we do any of that, we need to start with a tidy data. and that will be out topic for the next video."),(0,d.kt)("h2",{id:"tidy-data-and-data-structures"},"Tidy Data and Data structures"),(0,d.kt)("blockquote",null,(0,d.kt)("p",{parentName:"blockquote"},"Happy families are all alike; every unhappy family is unhappy in its own way"),(0,d.kt)("p",{parentName:"blockquote"},(0,d.kt)("em",{parentName:"p"},"Leo Tolstoy"))),(0,d.kt)("p",null,"That's how Leo Tolstoy started his Anna Karenina Novel. So how does it relate to us here in data analysis?"),(0,d.kt)("p",null,"Well it turns out, that we have some defined characteristics of what would be deemed as tidy data for example in Rectangular Data Structures: "),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Every variable/attribute/feature forms a column"),(0,d.kt)("li",{parentName:"ul"},"Every observation forms a row"),(0,d.kt)("li",{parentName:"ul"},"Every type of observational unit forms a table")),(0,d.kt)("p",null,"But if we think about what could be considered an untidy table, there's too many possibilities"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"you could have those merged rows that act as separators"),(0,d.kt)("li",{parentName:"ul"},"You could have tally rows"),(0,d.kt)("li",{parentName:"ul"},"You could have column headers that are actually a table description")),(0,d.kt)("p",null,"All of these are issues we have to tidy up before we can start with our analysis."),(0,d.kt)("p",null,"But Data generally don't come in this structured state: text and images for example, they first need to be processed and manipulated so that it can be represented as a set of features in tabular structure."),(0,d.kt)("p",null,"Let's take an picture example, at the end of the day, pictures are just pixels. where every pixel has an attribute of RGB, for colored pictures, and maybe on and off for black and white pictures."),(0,d.kt)("p",null,"If we wanted to represent this image in rectangular data structure/a data frame: and Let's assume that 0 means white, 1 means red, we can have this table here. However would this fit the description as tidy data?"),(0,d.kt)("p",null,"Well, no, rows and columns here basically a matrix, you'd want to convert that to table that has X, Y, and is_red as columns."),(0,d.kt)("p",null,"Table structured data is only one common forms of structured data; by no means we're saying it's only one.; what would be tidy for one analysis project may not be for another. For example, spatial data, which are used in mapping and location analytics.\nOr graph and network data that are used to analyze relations between entities like social network relations."),(0,d.kt)("p",null,"so by no means we're saying that rectangular data structures are the only acceptable forms of structured data for analysis. Depending on your project, you'll want to choose the right form for your data."),(0,d.kt)("p",null,"What we've done here; converting this image or this raw source of information into a dataset is what's called data wrangling.\nConverting this image to a dataset, parsing server log files to analyze web application traffic. In constructing the dataset, we also define the data types for each of the features. "),(0,d.kt)("p",null,"So looking at these 2 examples, you could see 2 different data types:\nNumerical and Categorical"),(0,d.kt)("p",null,"Let's talk about that in the next video."),(0,d.kt)("h2",{id:"data-types"},"Data Types"),(0,d.kt)("p",null,"There are 2 basic types of structured data:"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Numerical ",(0,d.kt)("ul",{parentName:"li"},(0,d.kt)("li",{parentName:"ul"},"Continuous"),(0,d.kt)("li",{parentName:"ul"},"Discrete"))),(0,d.kt)("li",{parentName:"ul"},"Categorical",(0,d.kt)("ul",{parentName:"li"},(0,d.kt)("li",{parentName:"ul"},"Ordinal"),(0,d.kt)("li",{parentName:"ul"},"Nominal")))),(0,d.kt)("p",null,"Numerical values can be continuous, as it taking any value in an interval, like height, weight, and temperature; you can have 30.345 inches, 150.57 pounds, and 98.6 degrees Fahrenheit.\nOr discrete; data that can only take integer values like number of children, number of cars, and number of pets. you can't have 1.5 children, 2.5 cars, or 1.5 pets."),(0,d.kt)("p",null,"and the other type of data is categorical: These are data that can only take specific set of values, representing possible categories."),(0,d.kt)("p",null,"For example, genders, and countries of athletes, rating scales on customer satisfaction surveys, letter grades of students in a class.\nCategorical data are split into nominal data, theses are just labels, or ordinal data, where the order matters or carries a meaning. Letter Grades, A is better then B which is better than a C, a rating scale of 5 is better than one. However, the white color is not better then blue, that's ordinal data."),(0,d.kt)("h3",{id:"why-do-we-care"},"Why do we care?"),(0,d.kt)("p",null,"Why do we care about data types? Well, it's important to know what type of data you're dealing with, because different types of data require different types of analysis. You have to analyze continuous data differently than categorical data otherwise it would result in a wrong analysis."),(0,d.kt)("p",null,"Let's remember a few key terms of summary statistics, you may recall that we have 2 types of summary statistics:"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Central tendency measures: mean, median, mode"),(0,d.kt)("li",{parentName:"ul"},"and Dispersion measures: variance, standard deviation, range")),(0,d.kt)("p",null,"For now, let's just focus on the central tendency measures, and let's look at the difference between mean and mode."),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Mean or the average: the sum of all the values divided by the number of values. "),(0,d.kt)("li",{parentName:"ul"},"Mode: the most commonly occurring category or value in data set. - which only makes sense for categorical data.")),(0,d.kt)("p",null,"So it's obvious that the mean is a summary statistic for continuous data, but the mode is a summary statistic for categorical data.\nFor a dataset of olympics athletes, you can get the mean of their ages, the mean of their heights, the mean of their weights, for example. It doesn't make sense to take the get the mean of their nationalities, of their sex. but you can get the mode of that."),(0,d.kt)("p",null,"Correctly computing Central Tendency measures becomes especially important when working on data cleaning and handling missing data as we'll talk about later."),(0,d.kt)("p",null,"Another example for why we care about data types is when we're talking about data visualization. Categorical data can be visualized using bar charts, pie charts, histograms, and so on. Continuous data can be visualized using line charts, scatter plots, and so on."),(0,d.kt)("p",null,"Also, when we're talking about data modeling and predictive analysis, we'll talk more about that later in the course, if you're trying to predict a continuous value like the price of a house, you'll use a regression model.\nYou'd use a logistical regression model if you're trying to predict predict whether a political candidate will win or lose an election, or a classification model if you're trying to predict whether a customer will buy a product or not."),(0,d.kt)("p",null,(0,d.kt)("img",{alt:"Linear Regression vs Logistic Regression",src:n(22496).Z,width:"998",height:"539"})),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"import pandas as pd\n\n# read_table and read_csv are the same except for the default delimiter\nauto_mpg_df = pd.read_table('./data/auto-mpg/auto-mpg.data', sep=\"\\t\", header=None, names=('mpg', 'cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name'))\n")),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"#df.head() gets the first 5 rows, sample(5) gets 5 random rows\nauto_mpg_df.sample(n=5)\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n      <th>car_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>114</th>\n      <td>12.0</td>\n      <td>8.0</td>\n      <td>350.0</td>\n      <td>180.0</td>\n      <td>4499.0</td>\n      <td>12.5</td>\n      <td>73.0</td>\n      <td>1.0</td>\n      <td>oldsmobile vista cruiser</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>19.0</td>\n      <td>6.0</td>\n      <td>232.0</td>\n      <td>100.0</td>\n      <td>2634.0</td>\n      <td>13.0</td>\n      <td>71.0</td>\n      <td>1.0</td>\n      <td>amc gremlin</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>30.0</td>\n      <td>4.0</td>\n      <td>97.0</td>\n      <td>67.0</td>\n      <td>1985.0</td>\n      <td>16.4</td>\n      <td>77.0</td>\n      <td>3.0</td>\n      <td>subaru dl</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>250.0</td>\n      <td>100.0</td>\n      <td>3278.0</td>\n      <td>18.0</td>\n      <td>73.0</td>\n      <td>1.0</td>\n      <td>chevrolet nova custom</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>24.0</td>\n      <td>4.0</td>\n      <td>134.0</td>\n      <td>96.0</td>\n      <td>2702.0</td>\n      <td>13.5</td>\n      <td>75.0</td>\n      <td>3.0</td>\n      <td>toyota corona</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# df.info() gives a summary about the data: number of rows, number of columns, column names, data types, number of non-null values\nauto_mpg_df.info()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 409 entries, 0 to 408\n    Data columns (total 9 columns):\n     #   Column        Non-Null Count  Dtype  \n    ---  ------        --------------  -----  \n     0   mpg           400 non-null    float64\n     1   cylinders     409 non-null    float64\n     2   displacement  409 non-null    float64\n     3   horsepower    403 non-null    float64\n     4   weight        409 non-null    float64\n     5   acceleration  409 non-null    float64\n     6   model_year    409 non-null    float64\n     7   origin        409 non-null    float64\n     8   car_name      409 non-null    object \n    dtypes: float64(8), object(1)\n    memory usage: 28.9+ KB\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# df.describe() gives statistical summaries of the data\nauto_mpg_df.describe()\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>400.000000</td>\n      <td>409.000000</td>\n      <td>409.000000</td>\n      <td>403.000000</td>\n      <td>409.000000</td>\n      <td>409.000000</td>\n      <td>409.000000</td>\n      <td>409.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>23.482000</td>\n      <td>5.488998</td>\n      <td>195.759169</td>\n      <td>105.429280</td>\n      <td>2986.088020</td>\n      <td>15.496577</td>\n      <td>75.897311</td>\n      <td>1.564792</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.810255</td>\n      <td>1.715129</td>\n      <td>106.809049</td>\n      <td>38.959819</td>\n      <td>848.259456</td>\n      <td>2.812496</td>\n      <td>3.754633</td>\n      <td>0.796027</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>9.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>46.000000</td>\n      <td>1613.000000</td>\n      <td>8.000000</td>\n      <td>70.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>17.375000</td>\n      <td>4.000000</td>\n      <td>105.000000</td>\n      <td>76.000000</td>\n      <td>2228.000000</td>\n      <td>13.600000</td>\n      <td>73.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>23.000000</td>\n      <td>4.000000</td>\n      <td>151.000000</td>\n      <td>95.000000</td>\n      <td>2833.000000</td>\n      <td>15.500000</td>\n      <td>76.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>29.000000</td>\n      <td>8.000000</td>\n      <td>302.000000</td>\n      <td>130.000000</td>\n      <td>3630.000000</td>\n      <td>17.100000</td>\n      <td>79.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>46.600000</td>\n      <td>8.000000</td>\n      <td>502.000000</td>\n      <td>230.000000</td>\n      <td>5140.000000</td>\n      <td>24.800000</td>\n      <td>82.000000</td>\n      <td>3.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("p",null,"Just looking at the info and the sample data, we could already see a few issues:"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"the origin column is actually a categorical field not a numerical one.",(0,d.kt)("ul",{parentName:"li"},(0,d.kt)("li",{parentName:"ul"},"Also looking at the stats for it: count, min, max, ... it looks like it can't only carry a value of 1, 2, or 3."),(0,d.kt)("li",{parentName:"ul"},"We can look at the data, and replace it with origin names, like US, Europe, and Asia."))),(0,d.kt)("li",{parentName:"ul"},"The model_year column is actually a ",(0,d.kt)("em",{parentName:"li"},"You tell me in the quiz")),(0,d.kt)("li",{parentName:"ul"},"The cylinders column is a continuous numerical data, but it is actually a ",(0,d.kt)("em",{parentName:"li"},"You tell me in the quiz"))),(0,d.kt)("p",null,"So not only are we doing data transformation, we're also doing some data wrangling and cleaning here."),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# The decimal point will be confusion to someone seeing the data\nauto_mpg_df['origin'] = auto_mpg_df['origin'].astype(int)\n# Make Origin a categorical variable\nauto_mpg_df['origin'] = pd.Categorical(auto_mpg_df.origin)\nauto_mpg_df.info()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 409 entries, 0 to 408\n    Data columns (total 9 columns):\n     #   Column        Non-Null Count  Dtype   \n    ---  ------        --------------  -----   \n     0   mpg           400 non-null    float64 \n     1   cylinders     409 non-null    float64 \n     2   displacement  409 non-null    float64 \n     3   horsepower    403 non-null    float64 \n     4   weight        409 non-null    float64 \n     5   acceleration  409 non-null    float64 \n     6   model_year    409 non-null    float64 \n     7   origin        409 non-null    category\n     8   car_name      409 non-null    object  \n    dtypes: category(1), float64(7), object(1)\n    memory usage: 26.2+ KB\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Assuming for my analysis project, I want cylinders to be a numerical variable. In that case, it would be better to be a discrete variable\nauto_mpg_df['cylinders'] = auto_mpg_df['cylinders'].astype('int')\nauto_mpg_df.info()\n\n# <Was that a good decision?>\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 409 entries, 0 to 408\n    Data columns (total 9 columns):\n     #   Column        Non-Null Count  Dtype   \n    ---  ------        --------------  -----   \n     0   mpg           400 non-null    float64 \n     1   cylinders     409 non-null    int64   \n     2   displacement  409 non-null    float64 \n     3   horsepower    403 non-null    float64 \n     4   weight        409 non-null    float64 \n     5   acceleration  409 non-null    float64 \n     6   model_year    409 non-null    float64 \n     7   origin        409 non-null    category\n     8   car_name      409 non-null    object  \n    dtypes: category(1), float64(6), int64(1), object(1)\n    memory usage: 26.2+ KB\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# right now, model year is a to be a numerical field, and we realized that it's represented as values of 70, 71, 72, etc. If we want to add cars from the yar 2022, this column would be very confusing. and we so wee need to change those value by adding 1900 to each value.\n# Please keep in mind, I'm confirming or denying the assumption that model year is a numerical variable. <You'll tell me that in the quiz>\n\nauto_mpg_df['model_year'] = auto_mpg_df['model_year'] + 1900\nauto_mpg_df\n\n# Also you could do:\n# auto_mpg_df['model_year'] = auto_mpg_df['model_year'].map(lambda x: x + 1900)\n# OR\n# auto_mpg_df['model_year'] = auto_mpg_df['model_year'].apply(lambda x: x + 1900)\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n      <th>car_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>307.0</td>\n      <td>130.0</td>\n      <td>3504.0</td>\n      <td>12.0</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>chevrolet chevelle malibu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>3693.0</td>\n      <td>11.5</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>buick skylark 320</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3436.0</td>\n      <td>11.0</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>plymouth satellite</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16.0</td>\n      <td>8</td>\n      <td>304.0</td>\n      <td>150.0</td>\n      <td>3433.0</td>\n      <td>12.0</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>amc rebel sst</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140.0</td>\n      <td>3449.0</td>\n      <td>10.5</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>ford torino</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>404</th>\n      <td>27.0</td>\n      <td>4</td>\n      <td>140.0</td>\n      <td>86.0</td>\n      <td>2790.0</td>\n      <td>15.6</td>\n      <td>1982.0</td>\n      <td>1</td>\n      <td>ford mustang gl</td>\n    </tr>\n    <tr>\n      <th>405</th>\n      <td>44.0</td>\n      <td>4</td>\n      <td>97.0</td>\n      <td>52.0</td>\n      <td>2130.0</td>\n      <td>24.6</td>\n      <td>1982.0</td>\n      <td>2</td>\n      <td>vw pickup</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>32.0</td>\n      <td>4</td>\n      <td>135.0</td>\n      <td>84.0</td>\n      <td>2295.0</td>\n      <td>11.6</td>\n      <td>1982.0</td>\n      <td>1</td>\n      <td>dodge rampage</td>\n    </tr>\n    <tr>\n      <th>407</th>\n      <td>28.0</td>\n      <td>4</td>\n      <td>120.0</td>\n      <td>79.0</td>\n      <td>2625.0</td>\n      <td>18.6</td>\n      <td>1982.0</td>\n      <td>1</td>\n      <td>ford ranger</td>\n    </tr>\n    <tr>\n      <th>408</th>\n      <td>31.0</td>\n      <td>4</td>\n      <td>119.0</td>\n      <td>82.0</td>\n      <td>2720.0</td>\n      <td>19.4</td>\n      <td>1982.0</td>\n      <td>1</td>\n      <td>chevy s-10</td>\n    </tr>\n  </tbody>\n</table>\n<p>409 rows \xd7 9 columns</p>\n</div>\n'))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_df.info()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 409 entries, 0 to 408\n    Data columns (total 9 columns):\n     #   Column        Non-Null Count  Dtype   \n    ---  ------        --------------  -----   \n     0   mpg           400 non-null    float64 \n     1   cylinders     409 non-null    int64   \n     2   displacement  409 non-null    float64 \n     3   horsepower    403 non-null    float64 \n     4   weight        409 non-null    float64 \n     5   acceleration  409 non-null    float64 \n     6   model_year    409 non-null    float64 \n     7   origin        409 non-null    category\n     8   car_name      409 non-null    object  \n    dtypes: category(1), float64(6), int64(1), object(1)\n    memory usage: 26.2+ KB\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Merge the 2 dataframes, when you already have the another dataframe; I already showed this in a pervious notebook, so i won't run it.\n# origins_df = pd.read_csv('./data/auto-mpg/origin.csv');\n\n# auto_mpg_df = pd.merge(auto_mpg_df, origins_df, on='origin')\n# auto_mpg_df.sample(5)\n")),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_df['origin_spelled'] = auto_mpg_df['origin'].astype(str).map({'1':'US', '2':'European', '3': 'Asian'})\n")),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_df.info()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 409 entries, 0 to 408\n    Data columns (total 10 columns):\n     #   Column          Non-Null Count  Dtype   \n    ---  ------          --------------  -----   \n     0   mpg             400 non-null    float64 \n     1   cylinders       409 non-null    int64   \n     2   displacement    409 non-null    float64 \n     3   horsepower      403 non-null    float64 \n     4   weight          409 non-null    float64 \n     5   acceleration    409 non-null    float64 \n     6   model_year      409 non-null    float64 \n     7   origin          409 non-null    category\n     8   car_name        409 non-null    object  \n     9   origin_spelled  409 non-null    object  \n    dtypes: category(1), float64(6), int64(1), object(2)\n    memory usage: 29.4+ KB\n"))),(0,d.kt)("p",null,"you will learn a lot more about this topic the more you work with data.\nBut for now, you know the data types, we do we care about them, and how to tidy up your data to move on with the analysis."),(0,d.kt)("h2",{id:"data-cleaning"},"Data Cleaning"),(0,d.kt)("p",null,"Now that we know about the data types we have in our data, we can start cleaning our data. Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset."),(0,d.kt)("p",null,"We start we a tidy dataset, we examine it, investigate it, and do some corrective actions to it, and and we end up with a clean dataset.\nSo what sorts of of issues do we clean up? Well, there's a lot of them, Like Leo Tolstoy mentioned, every family is unhappy in its own way, so every dataset is dirty in its own way. but we'll focus on the most common ones:"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Duplicate data"),(0,d.kt)("li",{parentName:"ul"},"Missing data"),(0,d.kt)("li",{parentName:"ul"},"Anomalies and outliers")),(0,d.kt)("h3",{id:"duplicate-data"},"Duplicate Data"),(0,d.kt)("p",null,"Let's start with duplicate data. Duplicate data is data that has been entered more than once. For example, if you have a dataset of customers, and you have a customer that has been entered twice, or more than twice, that's duplicate data. Or maybe you have a customer who filled out the same survey twice. That's duplicate data."),(0,d.kt)("p",null,"Duplicate data is a problem because it can skew your analysis, give the false impression that there's more data than there actually is.\nSo how do we deal with duplicate data? Well, we can remove it, or we can keep it.\nIf you have a dataset of customers, and you have a customer that has been entered twice, you can remove one of them or merge the records, or you can keep them both.\nIt depends on the situation. In a huge dataset, a few duplicate records, may not do much. But if you're merging the data with another, that could cause problems. It really depends on your project and the problem space you're working on."),(0,d.kt)("p",null,"So let's see how we can find and handle duplicate records."),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},(0,d.kt)("a",{parentName:"li",href:"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html"},"Pandas.DataFrame.duplicated "))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# this returns a series of booleans. You can pass some conditions to the functions, as to what needs to be considered in the comparison, and which record to be marked.\nauto_mpg_df.duplicated()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    0      False\n    1      False\n    2      False\n    3      False\n    4      False\n           ...  \n    404    False\n    405    False\n    406    False\n    407    False\n    408    False\n    Length: 409, dtype: bool\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Because it's truncated for display, we can get the summation\nauto_mpg_df.duplicated().sum()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    2\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# We can also filter the dataframe based on the results of the duplicated function\nauto_mpg_df[auto_mpg_df.duplicated()]\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n      <th>car_name</th>\n      <th>origin_spelled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>255</th>\n      <td>NaN</td>\n      <td>8</td>\n      <td>383.0</td>\n      <td>175.0</td>\n      <td>4166.0</td>\n      <td>10.5</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>plymouth satellite (sw)</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>16.0</td>\n      <td>8</td>\n      <td>400.0</td>\n      <td>180.0</td>\n      <td>4220.0</td>\n      <td>11.1</td>\n      <td>1977.0</td>\n      <td>1</td>\n      <td>pontiac grand prix lj</td>\n      <td>US</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Filter out the duplicates from the dataframe based on the results of the duplicated function\nauto_mpg_without_duplicates1 = auto_mpg_df[~auto_mpg_df.duplicated()]\n# Let's check now?\nauto_mpg_without_duplicates1.duplicated().sum()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    0\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# we didn't change the original dataframe, to do that, use the \nauto_mpg_df.duplicated().sum()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    2\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_without_duplicates2 = auto_mpg_df.drop_duplicates()\n# Let's check now?\nauto_mpg_without_duplicates2.duplicated().sum()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    0\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# to change the original dataframe, use the inplace parameter\nauto_mpg_df.drop_duplicates(inplace=True)\n# OR\n# auto_mpg_df = auto_mpg_df.drop_duplicates()\n")),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_df.info()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    <class 'pandas.core.frame.DataFrame'>\n    Int64Index: 407 entries, 0 to 408\n    Data columns (total 10 columns):\n     #   Column          Non-Null Count  Dtype   \n    ---  ------          --------------  -----   \n     0   mpg             399 non-null    float64 \n     1   cylinders       407 non-null    int64   \n     2   displacement    407 non-null    float64 \n     3   horsepower      401 non-null    float64 \n     4   weight          407 non-null    float64 \n     5   acceleration    407 non-null    float64 \n     6   model_year      407 non-null    float64 \n     7   origin          407 non-null    category\n     8   car_name        407 non-null    object  \n     9   origin_spelled  407 non-null    object  \n    dtypes: category(1), float64(6), int64(1), object(2)\n    memory usage: 32.3+ KB\n"))),(0,d.kt)("h3",{id:"missing-data-anamolies-and-outliers"},"Missing Data, Anamolies, and Outliers"),(0,d.kt)("p",null,"In working with data, you will have to deal with missing data, anomalies, and outliers."),(0,d.kt)("p",null,"Missing data is when you have a record, with a number of properties and some of those property values are missing. Maybe even values are are clearly incorrect. Like an age field of 0.\nAnomalies and outliers are for example when you have a record that seems incorrect, but may actually be true. Like, if I tell you that Bangladesh, the country that is slightly bigger than the state of New York, has a more people than Russia. That's an anomaly, but it's true. Or maybe you have a record of a person who's 5 feet tall, and weighs 300 pounds. That's an outlier, but could be true.\nand sometime you'll have to ask yourself the question: Is this an outlier or an anamoly that would mess up or skew your result, or is it a key finding or key observation."),(0,d.kt)("p",null,"or so how we handle those data issues: you could remove the records altogether, or you could substitute the data with some other value.\nYour project is what would determine the right approach here."),(0,d.kt)("p",null,"Also, This other value needs to fit within the dataset, not cause issues itself. It shouldn't skew the results itself. so what would this value be?"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Handling of Missing data",(0,d.kt)("ul",{parentName:"li"},(0,d.kt)("li",{parentName:"ul"},"Do you remove those records altogether, do you substitute those missing data with some other value",(0,d.kt)("ul",{parentName:"li"},(0,d.kt)("li",{parentName:"ul"},"what would that value be? We need what we call a typical value?")))))),(0,d.kt)("p",null,"This is where Statistics comes to the rescue once more. Central tendency measures along with some data visualizations would help us find those values. Depending on the type of the data, we could replace the data with the mean or the mode. In some cases, the median could make more sense to use as well."),(0,d.kt)("p",null,"There are different variations of the mean values that we can compute: We already talked about the normal mean, but I also want to introduce you to the another variation here, the trimmed mean.\nThis is when you calculate it by dropping a fixed number of sorted values at each end and then taking an average of the remaining values. A trimmed mean eliminates the influence of extreme values. For example, in international diving the top score and bottom score from five judges are dropped, and the final score is the average of the scores from the three remaining judges. This makes it difficult for a single judge to manipulate the score, perhaps to favor their country\u2019s contestant."),(0,d.kt)("p",null,"In some cases, using the median would be a better choice. The median is the middle number on a sorted list of the data. Compared to the mean, which uses all observations, the median depends only on the values in the center of the sorted data. Let\u2019s say we want to look at typical household incomes in neighborhoods around Lake Washington in Seattle. In comparing the Medina neighborhood to the Windermere neighborhood, using the mean would produce very different results because Bill Gates lives in Medina. If we use the median, it won\u2019t matter how rich Bill Gates is\u2014the position of the middle observation will remain the same. and the median here would be more representative of the typical value."),(0,d.kt)("p",null,"The median is not influenced by outliers, so it's a more robust estimate of location."),(0,d.kt)("p",null,"You should be able to use simple pandas operations to substitute those values."),(0,d.kt)("p",null,"But let's see how we can get those values in the first place."),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_df.describe()\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>399.000000</td>\n      <td>407.000000</td>\n      <td>407.000000</td>\n      <td>401.000000</td>\n      <td>407.000000</td>\n      <td>407.000000</td>\n      <td>407.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>23.500752</td>\n      <td>5.476658</td>\n      <td>194.797297</td>\n      <td>105.069825</td>\n      <td>2980.157248</td>\n      <td>15.519656</td>\n      <td>1975.909091</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.811039</td>\n      <td>1.710247</td>\n      <td>106.180724</td>\n      <td>38.721120</td>\n      <td>846.093527</td>\n      <td>2.799904</td>\n      <td>3.752056</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>9.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>46.000000</td>\n      <td>1613.000000</td>\n      <td>8.000000</td>\n      <td>1970.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>17.500000</td>\n      <td>4.000000</td>\n      <td>104.500000</td>\n      <td>76.000000</td>\n      <td>2227.000000</td>\n      <td>13.700000</td>\n      <td>1973.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>23.000000</td>\n      <td>4.000000</td>\n      <td>151.000000</td>\n      <td>95.000000</td>\n      <td>2830.000000</td>\n      <td>15.500000</td>\n      <td>1976.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>29.000000</td>\n      <td>8.000000</td>\n      <td>302.000000</td>\n      <td>130.000000</td>\n      <td>3616.500000</td>\n      <td>17.150000</td>\n      <td>1979.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>46.600000</td>\n      <td>8.000000</td>\n      <td>502.000000</td>\n      <td>230.000000</td>\n      <td>5140.000000</td>\n      <td>24.800000</td>\n      <td>1982.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"from scipy.stats import trim_mean\n\nauto_displacement_mean = auto_mpg_df['displacement'].mean()\nauto_displacement_trimmed_mean = trim_mean(auto_mpg_df['displacement'], 0.1)\nauto_displacement_median = auto_mpg_df['displacement'].median()\n\nprint(\"mean is {}, trimmed mean is {}, median is {}\".format(auto_displacement_mean, auto_displacement_trimmed_mean, auto_displacement_median))\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    mean is 194.7972972972973, trimmed mean is 184.2584097859327, median is 151.0\n"))),(0,d.kt)("p",null,"Next we need to find the missing fields and decide how we want to handle them"),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Find the missing data\nauto_mpg_df.isnull().sum()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    mpg               8\n    cylinders         0\n    displacement      0\n    horsepower        6\n    weight            0\n    acceleration      0\n    model_year        0\n    origin            0\n    car_name          0\n    origin_spelled    0\n    dtype: int64\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Looks like we have  8 missing values in the mpg column and 6 missing values in the horsepower column\n# Let's see what those rows look like\nauto_mpg_df[auto_mpg_df['mpg'].isnull()]\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n      <th>car_name</th>\n      <th>origin_spelled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>NaN</td>\n      <td>4</td>\n      <td>133.0</td>\n      <td>115.0</td>\n      <td>3090.0</td>\n      <td>17.5</td>\n      <td>1970.0</td>\n      <td>2</td>\n      <td>citroen ds-21 pallas</td>\n      <td>European</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>NaN</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>4142.0</td>\n      <td>11.5</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>chevrolet chevelle concours (sw)</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>NaN</td>\n      <td>8</td>\n      <td>351.0</td>\n      <td>153.0</td>\n      <td>4034.0</td>\n      <td>11.0</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>ford torino (sw)</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>NaN</td>\n      <td>8</td>\n      <td>383.0</td>\n      <td>175.0</td>\n      <td>4166.0</td>\n      <td>10.5</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>plymouth satellite (sw)</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NaN</td>\n      <td>8</td>\n      <td>360.0</td>\n      <td>175.0</td>\n      <td>3850.0</td>\n      <td>11.0</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>amc rebel sst (sw)</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>NaN</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140.0</td>\n      <td>3353.0</td>\n      <td>8.0</td>\n      <td>1970.0</td>\n      <td>1</td>\n      <td>ford mustang boss 302</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>NaN</td>\n      <td>4</td>\n      <td>97.0</td>\n      <td>48.0</td>\n      <td>1978.0</td>\n      <td>20.0</td>\n      <td>1971.0</td>\n      <td>2</td>\n      <td>volkswagen super beetle 117</td>\n      <td>European</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>NaN</td>\n      <td>4</td>\n      <td>121.0</td>\n      <td>110.0</td>\n      <td>2800.0</td>\n      <td>15.4</td>\n      <td>1981.0</td>\n      <td>2</td>\n      <td>saab 900s</td>\n      <td>European</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Assuming the analytics project here involves a step to predict the mpg of a car, we can't use these rows. So we'll drop them\nauto_mpg_df.dropna(subset=['mpg'], inplace=True)\n\n#Then we confirm that we don't have any missing values in the mpg column\nauto_mpg_df.isnull().sum()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    mpg               0\n    cylinders         0\n    displacement      0\n    horsepower        6\n    weight            0\n    acceleration      0\n    model_year        0\n    origin            0\n    car_name          0\n    origin_spelled    0\n    dtype: int64\n"))),(0,d.kt)("p",null,"For the horsepower, and before we decide how to handle it, we need to calculate the central tendency measures and and visualize it to see if we can find any patterns or anomalies in it. and to determine what would be the better central tendency measure to use to replace the missing values."),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_df['displacement'].describe()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    count    399.000000\n    mean     193.447368\n    std      105.563056\n    min        4.000000\n    25%      102.500000\n    50%      146.000000\n    75%      262.000000\n    max      502.000000\n    Name: displacement, dtype: float64\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},'import matplotlib as mpl\nimport matplotlib.pyplot as plt\n# plt.style.use("bmh")\n\n\nfig, ax = plt.subplots(figsize = (8,4))\n\nauto_mpg_df[\'displacement\'].plot(kind="hist", density= True, bins=30, alpha = 0.65)\nauto_mpg_df[\'displacement\'].plot(kind="kde")\n\nax.axvline(auto_displacement_mean, alpha = 0.8, linestyle = ":")\nax.axvline(auto_displacement_median, alpha = 0.8, linestyle = ":")\nax.axvline(auto_displacement_trimmed_mean, alpha = 0.8, linestyle = ":")\n\nax.set_ylim(0, .011)\nax.set_yticklabels([])\nax.set_ylabel("")\n\nax.text(auto_displacement_mean-.1, .01, "Mean", size = 10, alpha = 0.8)\nax.text(auto_displacement_median-.4, .0075, "Median", size = 10, alpha = 0.8)\nax.text(auto_displacement_trimmed_mean+.4, .0050, "Trimmed Mean", size = 10, alpha = 0.8)\n\nax.tick_params(left = False, bottom = False)\nfor ax, spine in ax.spines.items():\n    spine.set_visible(False)\n\nplt.show()\n')),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("p",null,(0,d.kt)("img",{alt:"png",src:n(33067).Z,width:"652",height:"355"}))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# We can can then use those values to substitute the missing data. so we'll use the median value to fill the missing values in the horsepower column\nauto_mpg_df['horsepower'].fillna(auto_mpg_df['horsepower'].median(), inplace=True)\n")),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"auto_mpg_df.isnull().sum()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    mpg               0\n    cylinders         0\n    displacement      0\n    horsepower        0\n    weight            0\n    acceleration      0\n    model_year        0\n    origin            0\n    car_name          0\n    origin_spelled    0\n    dtype: int64\n"))),(0,d.kt)("p",null,"You could also see a visual of all the features histograms in one plot, to see if there are any patterns or anomalies in the data, see the frequency of the values for categorical data, and where the the majority of the data is located for the numerical data."),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nauto_mpg_df.hist(bins=50, figsize=(20,15))\nplt.show()\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("p",null,(0,d.kt)("img",{alt:"png",src:n(81707).Z,width:"1600",height:"1222"}))),(0,d.kt)("p",null,"You could employ similar techniques to deal with outliers and anomalies. You could remove them, or you could replace them with some other value. But you need to be careful here, because you don't want to replace them with a value that would skew the results.\nOr you if your analysis is about finding the most performant cars, then you may want to keep those outliers in the dataset. In that case they're be considered key observations.\nWe use other statistical concepts from statistics to deal with outliers and anomalies, namely measures of variability and data distribution: standard deviation, variances, percentiles."),(0,d.kt)("p",null,"Please make sure you do the reading and the exercises for this section. It's very important to understand the concepts here."),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"## On outliers, here's how we can detect them\nauto_mpg_df['displacement'].plot(kind=\"box\")\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    <AxesSubplot: >\n")),(0,d.kt)("p",null,(0,d.kt)("img",{alt:"png",src:n(84462).Z,width:"554",height:"416"}))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# Once you have the box plot, you can use the whiskers to determine the outliers.\n# The whiskers are the lines that extend from the box. The top whisker is the 75th percentile, and the bottom whisker is the 25th percentile.\n# The outliers are the points that are outside the whiskers.\n\n# We can also use the describe function to get the 25th and 75th percentile\nauto_mpg_df['displacement'].describe()\n\n# We can also use the quantile function to get the 25th and 75th percentile\nauto_mpg_df['displacement'].quantile([0.25, 0.75])\n")),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    0.25    102.5\n    0.75    262.0\n    Name: displacement, dtype: float64\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# We can also use the IQR function to get the 25th and 75th percentile\nQ1 = auto_mpg_df['displacement'].quantile(0.25)\nQ3 = auto_mpg_df['displacement'].quantile(0.75)\nprint(\"Q1 is {} and Q3 is {}\".format(Q1, Q3))\nIQR = Q3 - Q1\nprint(\"IQR is {}\".format(IQR))\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nprint(\"The lower bound is {}, and the upper bound is {}\".format(lower_bound, upper_bound))\n\n## Records where the displacement values are outside the lower and upper bounds are outliers\nauto_mpg_df[(auto_mpg_df['displacement'] < lower_bound) | (auto_mpg_df['displacement'] > upper_bound)]\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"Q1 is 102.5 and Q3 is 262.0\nIQR is 159.5\nThe lower bound is -136.75, and the upper bound is 501.25\n")),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n      <th>car_name</th>\n      <th>origin_spelled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>43</th>\n      <td>18.0</td>\n      <td>6</td>\n      <td>502.0</td>\n      <td>100.0</td>\n      <td>3282.0</td>\n      <td>15.5</td>\n      <td>1971.0</td>\n      <td>1</td>\n      <td>outlier car</td>\n      <td>US</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"# That record doesn't look like a real data, so we'll drop it\nauto_mpg_df.drop(auto_mpg_df[(auto_mpg_df['displacement'] < lower_bound) | (auto_mpg_df['displacement'] > upper_bound)].index, inplace=True)\n")),(0,d.kt)("h2",{id:"about-statistical-analysis"},"About Statistical Analysis"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},(0,d.kt)("a",{target:"_blank",href:n(26e3).Z},"Statistics Cheatsheet"))),(0,d.kt)("p",null,"In this video we lightly touched on so many topics. "),(0,d.kt)("h2",{id:"data-visualization"},"Data Visualization"),(0,d.kt)("p",null,"I've mentioned to you before that for you do a good EDA, you need to\nLOOK AT THE DATA - SEE THE DATA - UNDERSTAND THE DATA"),(0,d.kt)("p",null,"Statistical analysis is very important for EDA and descriptive analysis, but it's not the only thing you need to do.\nIn fact, depending solely on statistical analysis can lead to wrong conclusions, and wrong analysis. It could lead to wrong data cleaning, and wrong data modeling.\nand we've seen how visualizations can help us in understanding the data. and cleaning it. "),(0,d.kt)("p",null,"Let's take a look at this dataset:"),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},'import pandas as pd\n\ndatasaurus = pd.read_table("./data/datasaurus.tsv")\ndatasaurus.head(5)\n')),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>dataset</th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dino</td>\n      <td>55.3846</td>\n      <td>97.1795</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dino</td>\n      <td>51.5385</td>\n      <td>96.0256</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dino</td>\n      <td>46.1538</td>\n      <td>94.4872</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dino</td>\n      <td>42.8205</td>\n      <td>91.4103</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dino</td>\n      <td>40.7692</td>\n      <td>88.3333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},'# Split the data into multiple dataframes\ndino_df = datasaurus[datasaurus["dataset"] == "dino"]\naway_df = datasaurus[datasaurus["dataset"] == "away"]\nh_lines_df = datasaurus[datasaurus["dataset"] == "h_lines"]\nv_lines_df = datasaurus[datasaurus["dataset"] == "v_lines"]\nx_shape_df = datasaurus[datasaurus["dataset"] == "x_shape"]\nstar_df = datasaurus[datasaurus["dataset"] == "star"]\nhigh_lines_df = datasaurus[datasaurus["dataset"] == "high_lines"]\ncircle_df = datasaurus[datasaurus["dataset"] == "circle"]\nbullseye_df = datasaurus[datasaurus["dataset"] == "bullseye"]\ndots_df = datasaurus[datasaurus["dataset"] == "dots"]\nslant_up_df = datasaurus[datasaurus["dataset"] == "slant_up"]\nslant_down_df = datasaurus[datasaurus["dataset"] == "slant_down"]\nwide_lines_df = datasaurus[datasaurus["dataset"] == "wide_lines"]\n')),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},'print("\\ndino_df")\nprint(dino_df.describe())\nprint("\\naway_df")\nprint(away_df.describe())\nprint("\\nh_lines_df")\nprint(h_lines_df.describe())\nprint("\\nv_lines_df")\nprint(v_lines_df.describe())\n')),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    \n    dino_df\n                    x           y\n    count  142.000000  142.000000\n    mean    54.263273   47.832253\n    std     16.765142   26.935403\n    min     22.307700    2.948700\n    25%     44.102600   25.288450\n    50%     53.333300   46.025600\n    75%     64.743600   68.525675\n    max     98.205100   99.487200\n    \n    away_df\n                    x           y\n    count  142.000000  142.000000\n    mean    54.266100   47.834721\n    std     16.769825   26.939743\n    min     15.560750    0.015119\n    25%     39.724115   24.625892\n    50%     53.340296   47.535269\n    75%     69.146597   71.803148\n    max     91.639961   97.475771\n    \n    h_lines_df\n                    x           y\n    count  142.000000  142.000000\n    mean    54.261442   47.830252\n    std     16.765898   26.939876\n    min     22.003709   10.463915\n    25%     42.293828   30.479911\n    50%     53.069678   50.473527\n    75%     66.768274   70.349471\n    max     98.288123   90.458936\n    \n    v_lines_df\n                    x           y\n    count  142.000000  142.000000\n    mean    54.269927   47.836988\n    std     16.769959   26.937684\n    min     30.449654    2.734760\n    25%     49.964506   22.752884\n    50%     50.362890   47.113616\n    75%     69.504068   65.845391\n    max     89.504851   99.694680\n"))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},'import matplotlib.pyplot as plt\n# import seaborn as sns\n# sns.set()\nplt.style.use("bmh")\n\nfig, (\n  (ax1,ax2,ax3),\n  (ax4,ax5,ax6),\n  (ax7,ax8,ax9),\n  (ax10,ax11,ax12),\n  (ax13,ax14,ax15)\n) = plt.subplots(5,3, figsize=(15, 15), sharex=True)\n\nax1.scatter(away_df["x"], away_df["y"])\nax1.set_title("away_df")\n\nax2.scatter(h_lines_df["x"], h_lines_df["y"])\nax2.set_title("h_lines_df")\n\nax3.scatter(v_lines_df["x"], v_lines_df["y"])\nax3.set_title("v_lines_df")\n\nax4.scatter(x_shape_df["x"], x_shape_df["y"])\nax4.set_title("x_shape_df")\n\nax5.scatter(star_df["x"], star_df["y"])\nax5.set_title("star_df")\n\nax6.scatter(high_lines_df["x"], high_lines_df["y"])\nax6.set_title("high_lines_df")\n\nax7.scatter(circle_df["x"], circle_df["y"])\nax7.set_title("circle_df")\n\nax8.scatter(bullseye_df["x"], bullseye_df["y"])\nax8.set_title("bullseye_df")\n\nax9.scatter(dots_df["x"], dots_df["y"])\nax9.set_title("dots_df")\n\nax10.scatter(slant_up_df["x"], slant_up_df["y"])\nax10.set_title("slant_up_df")\n\nax11.scatter(slant_down_df["x"], slant_down_df["y"])\nax11.set_title("slant_down_df")\n\nax12.scatter(wide_lines_df["x"], wide_lines_df["y"])\nax12.set_title("wide_lines_df")\n\nax13.scatter(dino_df["x"], dino_df["y"])\nax13.set_title("dino_df")\n')),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    Text(0.5, 1.0, 'dino_df')\n")),(0,d.kt)("p",null,(0,d.kt)("img",{alt:"png",src:n(5304).Z,width:"1216",height:"1220"}))),(0,d.kt)("h2",{id:"data-correlations"},"Data Correlations"),(0,d.kt)("p",null,"Exploratory data analysis involves examining correlation among features or properties or the columns and a target variable."),(0,d.kt)("p",null,"Later on, you'll learn that in machine learning, the amount of data you have matters a lot, and the more features you use, your more rows of observation records you need to have. for the model to be able to learn about the patterns between the different features.\nOne that that could help with that, is to find features that are highly correlated with each other, and remove one of them. This is called feature selection."),(0,d.kt)("p",null,"So what is correlation? Correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. It is used to identify the degree of relationship between two variables. The correlation coefficient is a statistical measure that indicates the strength of the relationship between two variables. It is a number between -1 and 1. A value of 1 indicates a perfect positive correlation, a value of -1 indicates a perfect negative correlation, and a value of 0 indicates no correlation at all. Correlation can be one of the measures used in inferential statistics to determine the strength of the relationship between two variables."),(0,d.kt)("p",null,"and as with everything in data science and data analysis, domain knowledge is needed to understand the results and not come up with bad conclusions."),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},(0,d.kt)("img",{parentName:"li",src:"https://i.gifer.com/MmOU.gif",alt:"How I met your mother - Driving Gloves"}))),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},"corr_matrix = auto_mpg_df.corr()\ncorr_matrix\n")),(0,d.kt)(h,{mdxType:"HTMLOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"/var/folders/c_/2lqspw9x5ts9mgzxq7drs2400000gq/T/ipykernel_55303/1551412334.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr_matrix = auto_mpg_df.corr()\n")),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mpg</th>\n      <td>1.000000</td>\n      <td>-0.775396</td>\n      <td>-0.788607</td>\n      <td>-0.773382</td>\n      <td>-0.831741</td>\n      <td>0.420289</td>\n      <td>0.579267</td>\n    </tr>\n    <tr>\n      <th>cylinders</th>\n      <td>-0.775396</td>\n      <td>1.000000</td>\n      <td>0.937337</td>\n      <td>0.841200</td>\n      <td>0.896017</td>\n      <td>-0.505419</td>\n      <td>-0.348746</td>\n    </tr>\n    <tr>\n      <th>displacement</th>\n      <td>-0.788607</td>\n      <td>0.937337</td>\n      <td>1.000000</td>\n      <td>0.876512</td>\n      <td>0.915268</td>\n      <td>-0.549899</td>\n      <td>-0.357416</td>\n    </tr>\n    <tr>\n      <th>horsepower</th>\n      <td>-0.773382</td>\n      <td>0.841200</td>\n      <td>0.876512</td>\n      <td>1.000000</td>\n      <td>0.862380</td>\n      <td>-0.686502</td>\n      <td>-0.413649</td>\n    </tr>\n    <tr>\n      <th>weight</th>\n      <td>-0.831741</td>\n      <td>0.896017</td>\n      <td>0.915268</td>\n      <td>0.862380</td>\n      <td>1.000000</td>\n      <td>-0.417457</td>\n      <td>-0.306564</td>\n    </tr>\n    <tr>\n      <th>acceleration</th>\n      <td>0.420289</td>\n      <td>-0.505419</td>\n      <td>-0.549899</td>\n      <td>-0.686502</td>\n      <td>-0.417457</td>\n      <td>1.000000</td>\n      <td>0.288137</td>\n    </tr>\n    <tr>\n      <th>model_year</th>\n      <td>0.579267</td>\n      <td>-0.348746</td>\n      <td>-0.357416</td>\n      <td>-0.413649</td>\n      <td>-0.306564</td>\n      <td>0.288137</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,d.kt)("p",null,"You could also get the correlation for a particular column. Let's say we want to see how the horsepower is correlated with the weight of the car. We could use the corr() method on the dataframe, and pass the column name as an argument to it."),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},'corr_matrix["horsepower"].sort_values(ascending=False)\n')),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    horsepower      1.000000\n    displacement    0.876512\n    weight          0.862380\n    cylinders       0.841200\n    model_year     -0.413649\n    acceleration   -0.686502\n    mpg            -0.773382\n    Name: horsepower, dtype: float64\n"))),(0,d.kt)("p",null,"Data like this could aid in feature selection and can be used to determine removing one feature from the dataset to reduce the number of features, and thus the number of rows of data needed to train the model."),(0,d.kt)("p",null,"Looking at the data above here, it might be suggested that there's a high correlation between the horsepower and the weight of the car. But can you get rid of one and keep the other? With domain knowledge, you could learn that while there may be some correlation, it's not where if you need to increase the horsepower, you increase the weight. It's more of Because the weight is high, the horsepower needs to be high as well."),(0,d.kt)("p",null,"Domain knowledge could also dismiss that as a probable sampling issue where the data may not be representative of the population. There may be light cars with high horse power as well. So you can't just remove a feature because it's highly correlated with another feature."),(0,d.kt)("p",null,"Let me show you another way where you can see this correlation visually."),(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre",className:"language-python"},'from pandas.plotting import scatter_matrix\n\nattributes = ["mpg", "weight", "displacement",\n              "horsepower", "cylinders"]\nscatter_matrix(auto_mpg_df[attributes], figsize=(12, 8))\n')),(0,d.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,d.kt)("pre",null,(0,d.kt)("code",{parentName:"pre"},"    array([[<AxesSubplot: xlabel='mpg', ylabel='mpg'>,\n            <AxesSubplot: xlabel='weight', ylabel='mpg'>,\n            <AxesSubplot: xlabel='displacement', ylabel='mpg'>,\n            <AxesSubplot: xlabel='horsepower', ylabel='mpg'>,\n            <AxesSubplot: xlabel='cylinders', ylabel='mpg'>],\n           [<AxesSubplot: xlabel='mpg', ylabel='weight'>,\n            <AxesSubplot: xlabel='weight', ylabel='weight'>,\n            <AxesSubplot: xlabel='displacement', ylabel='weight'>,\n            <AxesSubplot: xlabel='horsepower', ylabel='weight'>,\n            <AxesSubplot: xlabel='cylinders', ylabel='weight'>],\n           [<AxesSubplot: xlabel='mpg', ylabel='displacement'>,\n            <AxesSubplot: xlabel='weight', ylabel='displacement'>,\n            <AxesSubplot: xlabel='displacement', ylabel='displacement'>,\n            <AxesSubplot: xlabel='horsepower', ylabel='displacement'>,\n            <AxesSubplot: xlabel='cylinders', ylabel='displacement'>],\n           [<AxesSubplot: xlabel='mpg', ylabel='horsepower'>,\n            <AxesSubplot: xlabel='weight', ylabel='horsepower'>,\n            <AxesSubplot: xlabel='displacement', ylabel='horsepower'>,\n            <AxesSubplot: xlabel='horsepower', ylabel='horsepower'>,\n            <AxesSubplot: xlabel='cylinders', ylabel='horsepower'>],\n           [<AxesSubplot: xlabel='mpg', ylabel='cylinders'>,\n            <AxesSubplot: xlabel='weight', ylabel='cylinders'>,\n            <AxesSubplot: xlabel='displacement', ylabel='cylinders'>,\n            <AxesSubplot: xlabel='horsepower', ylabel='cylinders'>,\n            <AxesSubplot: xlabel='cylinders', ylabel='cylinders'>]],\n          dtype=object)\n")),(0,d.kt)("p",null,(0,d.kt)("img",{alt:"png",src:n(56561).Z,width:"1009",height:"694"}))),(0,d.kt)("p",null,"and from here you could see that the figure where the same property meets, is a histogram about the frequency of the data"),(0,d.kt)("p",null,(0,d.kt)("a",{parentName:"p",href:"https://courses.helsinki.fi/sites/default/files/course-material/4509270/IntroDS-03.pdf"},"https://courses.helsinki.fi/sites/default/files/course-material/4509270/IntroDS-03.pdf")),(0,d.kt)("h4",{id:"assignments"},"Assignments"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},"Lab assignment on EDA"),(0,d.kt)("li",{parentName:"ul"},"For a particular problem set by me, what kinds of questions would you ask? what data sources would you use?")),(0,d.kt)("h2",{id:"references"},"References"),(0,d.kt)("ul",null,(0,d.kt)("li",{parentName:"ul"},(0,d.kt)("a",{parentName:"li",href:"https://www.autodesk.com/research/publications/same-stats-different-graphs"},"Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing"))))}c.isMDXComponent=!0},26e3:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/files/statistics-cheat-sheet-0a050230ba78d7112d9bd44d1a729a0d.pdf"},33067:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_35_0-155b09afc646621c235f4d9171632de7.png"},81707:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_39_0-88860e7cf1073d8bbdca8070a758d943.png"},84462:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_41_1-b0cfcd6501696704ee5441e8fa8f19f1.png"},5304:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_51_1-b4cdd3f539a612ae3e20847c8bc0146c.png"},56561:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/output_57_1-83627af92d5d2beaf9e9ae51dabd7a42.png"},22496:(t,e,n)=>{n.d(e,{Z:()=>a});const a=n.p+"assets/images/linear-vs-logistic-regression-1706ed742448da1ecdf0f86c8bf368a0.png"}}]);