"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[2770],{4534:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var a=n(7462),l=(n(7294),n(3905));n(1839);const o={sidebar_position:1,title:"Regression",draft:!1},i=void 0,r={unversionedId:"ml-regression/index",id:"ml-regression/index",title:"Regression",description:"What is Regression?",source:"@site/content/course-notes/9.ml-regression/index.md",sourceDirName:"9.ml-regression",slug:"/ml-regression/",permalink:"/course-notes/ml-regression/",draft:!1,editUrl:"https://github.com/IT4063C/IT4063C.github.io/tree/main/content/course-notes/9.ml-regression/index.md",tags:[],version:"current",lastUpdatedBy:"Yahya Gilany",lastUpdatedAt:1666817930,formattedLastUpdatedAt:"Oct 26, 2022",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Regression",draft:!1},sidebar:"notes",previous:{title:"Machine Learning",permalink:"/course-notes/ml/"},next:{title:"Machine Learning - Classification",permalink:"/course-notes/classification/"}},s={},d=[{value:"What is Regression?",id:"what-is-regression",level:2},{value:"Types of Regression",id:"types-of-regression",level:2},{value:"The Goal of Regression",id:"the-goal-of-regression",level:2},{value:"Model and Notations",id:"model-and-notations",level:2},{value:"Cost Function",id:"cost-function",level:2},{value:"Gradient Descent",id:"gradient-descent",level:2},{value:"Using <code>Scikit-Learn</code>",id:"using-scikit-learn",level:2},{value:"Simple Linear Regression",id:"simple-linear-regression",level:3},{value:"Multiple Linear Regression",id:"multiple-linear-regression",level:3},{value:"Polynomial Regression",id:"polynomial-regression",level:3},{value:"Multiple Polynomial Regression",id:"multiple-polynomial-regression",level:3}],p=e=>function(t){return console.warn("Component "+e+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",t)},c=p("HTMLOutputBlock"),m=p("CodeOutputBlock"),h={toc:d};function u(e){let{components:t,...o}=e;return(0,l.kt)("wrapper",(0,a.Z)({},h,o,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h2",{id:"what-is-regression"},"What is Regression?"),(0,l.kt)("p",null,"Linear Regression and Polynomial Regression are supervised regression algorithms. Regression models are used to we predict numerical values instead of categories or classes."),(0,l.kt)("p",null,"Regression models finds relationships between:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"one or more independent variables, or explanatory variables,"),(0,l.kt)("li",{parentName:"ul"},"and a target variable or dependent variable.")),(0,l.kt)("h2",{id:"types-of-regression"},"Types of Regression"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Linear Regression",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Simple Linear Regression (2D)"),(0,l.kt)("li",{parentName:"ul"},"Multiple Linear Regression (3D or more)"))),(0,l.kt)("li",{parentName:"ul"},"Polynomial Regression",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Simple Polynomial Regression (2D)"),(0,l.kt)("li",{parentName:"ul"},"Multiple Polynomial Regression (3D or more)")))),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"regression types",src:n(7306).Z,width:"1167",height:"1274"})),(0,l.kt)("h2",{id:"the-goal-of-regression"},"The Goal of Regression"),(0,l.kt)("p",null,"given the following dataset. if we want to determine the income of a person based on their education and seniority. We can use a regression model that learns from historical data to create a model that describes the relationship between the education and seniority of a person and their income. and use that model to make estimations."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# import csv file\nimport pandas as pd\n\nincome_df = pd.read_csv('data/income2.csv')\nincome_df.head(6)\n")),(0,l.kt)(c,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>Education</th>\n      <th>Seniority</th>\n      <th>Income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21.586207</td>\n      <td>113.103448</td>\n      <td>99.917173</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18.275862</td>\n      <td>119.310345</td>\n      <td>92.579135</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.068966</td>\n      <td>100.689655</td>\n      <td>34.678727</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17.034483</td>\n      <td>187.586207</td>\n      <td>78.702806</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19.931034</td>\n      <td>20.000000</td>\n      <td>68.009922</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>18.275862</td>\n      <td>26.206897</td>\n      <td>71.504485</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("p",null,"a quick EDA, can show a strong positive correlation between years of education and income (0.9), and a less-strong positive correlation between seniority level and income (0.52)."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Visualize the correlation matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nincome_corr = income_df.corr()\nsns.heatmap(income_corr, annot=True, cmap='coolwarm')\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(2838).Z,width:"515",height:"418"}))),(0,l.kt)("p",null,"Correlation however doesn't tell us what type of relationship it is, and if it's linear or not.\nAlso, while the correlation between seniority and income may be less strong, it can be strong when used in combination with years of education. One of the tasks Feature Engineering help us with."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Visualize a scatter plot between income and education\nsns.scatterplot(x='Education', y='Income', data=income_df)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},"![png](_index_files/output_8_0.png)"),(0,l.kt)("p",null,"Visualizing the data using a scatter plot, shows us that there is a linear relationship between years of education and income."),(0,l.kt)("p",null,"Let's include, seniority level in the mix."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# visualize a 3d scatter plot between Education, Seniority, and Income using mpl_toolkits\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(5,5))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(income_df['Education'], income_df['Seniority'], income_df['Income'])\nax.set_xlabel('Education')\nax.set_ylabel('Seniority')\nax.set_zlabel('Income')\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(2451).Z,width:"426",height:"415"}))),(0,l.kt)("p",null,"a static 3d scatter plot is kind of hard to read, let's create an interactive one using plotly."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# visualize a 3d scatter plot between Education, Seniority, and Income using plotly\nimport plotly.express as px\n\n\nfig = px.scatter_3d(income_df, x='Education', y='Seniority', z='Income')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},""))),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"I couldn't find a way to make the plot interactive in the course note, so here's a gif"),(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("img",{alt:"3d scatter plot",src:n(5059).Z,width:"754",height:"450"}))),(0,l.kt)("p",null,"we can see a surface model that can cut through the data points. which suggest that we can build a multiple linear regression model here."),(0,l.kt)("p",null,"The goal of Regression Models is to find that line (for simple linear regression) or surface (for multiple linear regression) that best fits the data."),(0,l.kt)("h2",{id:"model-and-notations"},"Model and Notations"),(0,l.kt)("p",null,"Whenever you learn about a new machine learning algorithm, you should learn about the model and the notations used to describe it."),(0,l.kt)("p",null,"Generally, the data notations are described as follows:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Every dataset we work with has a number (m) of (records, observations, signals, instances, data point), these are all synonyms."),(0,l.kt)("li",{parentName:"ul"},"Each of those observations ia a vector of (features, independent variables, explanatory variables, predictors, dimensions, attributes) (x) and a for labelled data, or data we use in supervised learning we also have a target (y). These are also called targets, dependent variables, or a responses, classes, or a categories."),(0,l.kt)("li",{parentName:"ul"},"the number of features is (n). ")),(0,l.kt)("p",null,"so for this dataset, we can say that:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"we have 30 records, m = 30"),(0,l.kt)("li",{parentName:"ul"},"we have 2 features, n = 2"),(0,l.kt)("li",{parentName:"ul"},"features are years of education, and seniority level"),(0,l.kt)("li",{parentName:"ul"},"target is income")),(0,l.kt)("p",null,"The simple linear regression model is the same as the line equation:"),(0,l.kt)("p",null,"$$y = mx + b$$"),(0,l.kt)("p",null,"where:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"m: slope"),(0,l.kt)("li",{parentName:"ul"},"c: y-intercept or the bias term, or the value of y when x = 0")),(0,l.kt)("p",null,"but since we have different notation for what (m) is, we represent the simple linear regression model as:"),(0,l.kt)("p",null,"$$\\hat{y} = h(x) = \\theta",(0,l.kt)("em",{parentName:"p"},"{0} + \\theta"),"{1}x$$"),(0,l.kt)("p",null,"where:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"$\\hat{y}$: the predicted value, also called the hypothesis function $h(x)$"),(0,l.kt)("li",{parentName:"ul"},"$\\theta_{0}$: the bias term, or the y-intercept"),(0,l.kt)("li",{parentName:"ul"},"$\\theta_{1}$: the slope of the line")),(0,l.kt)("p",null,"Let's plot a few lines to remember the equation."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-5, 5, 11)\n\n# a funciton that takes a string equation describing a line and color parameter and plots the line\ndef plot_line(y, color):\n  y_ = eval(y)\n  plt.plot(x, y_, label=f'y={y}', marker='o', markersize=5, color=color)\n  plt.legend(loc='best')\n  plt.xlabel('x')\n  plt.ylabel('y')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Calling the plot line function with different parameters\nplot_line(y='x', color='red')\nplot_line(y='3*x+5', color='blue')\nplot_line(y='-2*x+4', color='green')\nplot_line(y='2*x', color='purple')\n\nplt.grid()\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(9006).Z,width:"574",height:"432"}))),(0,l.kt)("p",null,"Simple Linear Regression Model:\n$$\\hat{y} = h(x) = \\theta",(0,l.kt)("em",{parentName:"p"},"{0} + \\theta"),"{1}x$$"),(0,l.kt)("p",null,"where:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"$\\hat{y}$: the predicted value, also called the hypothesis function $h(x)$"),(0,l.kt)("li",{parentName:"ul"},"$\\theta_{0}$: the bias term, or the y-intercept"),(0,l.kt)("li",{parentName:"ul"},"$\\theta_{1}$: the slope of the line")),(0,l.kt)("p",null,"it's as if the $\\theta_{0}$ was multiplied by $x^{0} = 1$"),(0,l.kt)("p",null,"if we had an extra feature/dimension (n=2), the equation would be generalized to represent a plane or a surface as:"),(0,l.kt)("p",null,"$$\\hat{y} = h(x) = \\theta",(0,l.kt)("em",{parentName:"p"},"{0} + \\theta"),"{1}x",(0,l.kt)("em",{parentName:"p"},"{1} + \\theta"),"{2}x_{2}$$"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"fig = plt.figure()\n\nax = fig.add_subplot(111,projection='3d')\n\nx1, x2 = np.meshgrid(range(10), range(10))\n\ny_hat = (9 - x1 - x2)\n\nax.plot_surface(x1, x2, y_hat, alpha=0.5)\n\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(4238).Z,width:"410",height:"391"}))),(0,l.kt)("p",null,"if we had n features/dimensions, the equation would be generalized to represent a hyperplane or a hypersurface as:"),(0,l.kt)("p",null,"$$\\hat{y} = h(x) = \\theta",(0,l.kt)("em",{parentName:"p"},"{0} + \\theta"),"{1}x",(0,l.kt)("em",{parentName:"p"},"{1} + \\theta"),"{2}x",(0,l.kt)("em",{parentName:"p"},"{2} + ... + \\theta"),"{n}x_{n}$$"),(0,l.kt)("p",null,"or the vectorized form of this would be:"),(0,l.kt)("p",null,"$$\\hat{y} = h(x) = \\Theta^{T}X$$"),(0,l.kt)("p",null,"where:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"$X$: the matrix of features, or the matrix of independent variables"),(0,l.kt)("li",{parentName:"ul"},"$\\Theta$: the vector of parameters, or the vector of coefficients, the values we're adjusting to fit the model."),(0,l.kt)("li",{parentName:"ul"},"$\\hat{y}$: the predicted value, also called the hypothesis function $h(x)$")),(0,l.kt)("p",null,"Vectorized forms are a way to represent the same equation in a more compact way. Many mathematical libraries are more optimized to work with vectorized forms."),(0,l.kt)("p",null,"so how do we get the best values for $\\Theta$? how do we call one model better than another? how do we know if our model is good or not?"),(0,l.kt)("p",null,"We use Performance Measures/Cost Functions/Lost Functions to evaluate our model."),(0,l.kt)("h2",{id:"cost-function"},"Cost Function"),(0,l.kt)("p",null,"In simple linear regression, we aim to get a a line that can represent the data we have as best as possible."),(0,l.kt)("p",null,"If you had the following lines, which one would you choose? and why?"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# a pandas dataframe with 2 columns: x and y\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = [1, 1.5, 2.5, 3, 4]\ny = [1, 2.3, 2.1, 3, 2.9]\n\ndf = pd.DataFrame({'x': x, 'y': y})\n# plot the data\n\n# Scatter plot of the dots\nf, ax = plt.subplots(figsize=(5,5))\nax.scatter(df['x'], df['y'], s=60)\n\n# Plot an arbitrary line\nx = np.linspace(0, 4, 11)\ny_bad = 2.2+ -x\ny_good = x + 0.5\ny_better = 0.55789474*x + 0.92105263\nax.plot(x,y_bad, label=f'bad: y=2.2 - x', color='red')\nax.plot(x,y_good, label=f'good: y=0.5 + x', color='orange')\nax.plot(x,y_better, label=f'better: y= 0.92 + 0.558 x', color='green')\nax.legend(loc='best')\n\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(9618).Z,width:"437",height:"428"}))),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"We can't choose a line based on how it looks, we need to use a performance measure to evaluate the model."),(0,l.kt)("li",{parentName:"ul"},"We can't use arbitrary points to draw a line using the slope: $y = \\frac{y",(0,l.kt)("em",{parentName:"li"},"{2} - y"),"{1}}{x",(0,l.kt)("em",{parentName:"li"},"{2} - x"),"{1}}x + b$")),(0,l.kt)("p",null,"One of the common performance measures is mean of the squared errors or the squared residuals $MSE$. "),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"How?")," we take the difference between the actual value and the predicted value, and we square it. and we do that for every data point, and we get the mean of all those values."),(0,l.kt)("p",null,"$$MSE = \\frac{1}{m}\\sum",(0,l.kt)("em",{parentName:"p"},"{i=1}^{m}(y"),"{i} - \\hat{y_{i}})^{2}$$"),(0,l.kt)("p",null,"We want to find the $\\hat{y}$ that gets us the lowers $MSE$."),(0,l.kt)("p",null,"Here's an explanation of calculating the $MSE$ for a simple linear regression model. (we won't be doing this manually)"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import pandas as pd\n\ndf = pd.DataFrame([[1, 1], [1.5, 2.3], [2.5, 2.1], [3, 3], [4, 2.9]], columns=['x', 'y'])\ndf\n")),(0,l.kt)(c,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.5</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.5</td>\n      <td>2.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>2.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("p",null,"we'll just start with a random line, any line, "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# plot the data\nimport matplotlib.pyplot as plt\n\n# Scatter plot of the dots\nf, ax = plt.subplots(figsize=(5,5))\nax.scatter(df['x'], df['y'])\n\n# Plot an arbitrary line\nx = np.linspace(0, 4, 11)\ny = 0*x + 2.2\nax.plot(x,y, label=f'y=2.25')\n\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(285).Z,width:"447",height:"428"}))),(0,l.kt)("p",null,"and then calculate the distance between the line and the data points."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# plot the data\nimport matplotlib.pyplot as plt\n\n# Scatter plot of the dots\nf, ax = plt.subplots(figsize=(5,5))\nax.scatter(df['x'], df['y'])\n\n# Plot an arbitrary line\nx = np.linspace(0, 4, 11)\ny = 0*x + 2.2\nax.plot(x,y, label=f'y=2.25')\n\n# Residuals\nax.plot([1,1],[1, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.1, 1.5, '1.2')\nax.plot([1.5,1.5],[2.3, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.6, 2.25, '-0.1')\nax.plot([2.5,2.5],[2.1, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(2.6, 2.12, '0.1')\nax.plot([3,3],[3, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(3.1, 2.4, '-0.8')\nax.plot([4,4],[2.9, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(4.1, 2.6, '-0.7')\n\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(1132).Z,width:"465",height:"428"}))),(0,l.kt)("p",null,"Some of the values are negative, summing the errors may lead to them cancelling each other out, so we square the distance, and we sum all the squared distances, "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# plot the data\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nax.scatter(df['x'], df['y'])\n\nx = np.linspace(0, 4, 5)\ny = 0*x + 2.2\ny_ = 0*df['x'] + 2.2\n\nresiduals = (y_ - df['y'])**2\nSE = np.sum(residuals)\nMSE = np.mean(residuals)\n\nax.plot(x,y, label=f'y=2.25')\n\nax.plot([1,1],[1, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.1, 1.5, '1.44')\nax.plot([1.5,1.5],[2.3, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.6, 2.25, '0.01')\nax.plot([2.5,2.5],[2.1, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(2.6, 2.12, '0.01')\nax.plot([3,3],[3, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(3.1, 2.4, '0.64')\nax.plot([4,4],[2.9, 2.2], color='red', linestyle='dotted', alpha=0.5)\nax.text(4.1, 2.6, '0.49')\n\nax.text(2, 1.5, f'\u2211={SE}\\nMSE={MSE}', fontsize=20, color='red')\n\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(2666).Z,width:"469",height:"428"}))),(0,l.kt)("p",null,"$$MSE = \\frac{1}{m}\\sum",(0,l.kt)("em",{parentName:"p"},"{i=1}^{m}(y"),"{i} - \\hat{y_{i}})^{2}$$\nWe call this the loss function, or the cost function. and we want to minimize this function."),(0,l.kt)("p",null,"Rinse and Repeat until we get the best line."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# plot the data\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nax.scatter(df['x'], df['y'])\n\nx = np.linspace(0, 4, 5)\ny = x + 0.5\ny_ = df['x'] + 0.5\n\nresiduals = (y_ - df['y'])**2\nSE = np.sum(residuals)\nMSE = np.mean(residuals)\n\nax.plot(x,y, label=f'y=0.5 + x')\n\nax.plot([1,1],[1, 1.5], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.1, 1.5, '0.25')\nax.plot([1.5,1.5],[2.3, 2], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.6, 2.25, '0.09')\nax.plot([2.5,2.5],[2.1, 3], color='red', linestyle='dotted', alpha=0.5)\nax.text(2.6, 2.12, '0.81')\nax.plot([3,3],[3, 3.5], color='red', linestyle='dotted', alpha=0.5)\nax.text(3.1, 2.4, '0.25')\nax.plot([4,4],[2.9, 4.5], color='red', linestyle='dotted', alpha=0.5)\nax.text(4.1, 2.6, '2.56')\n\nax.text(1.5, .5, f'\u2211={SE}\\nMSE={MSE}', fontsize=20, color='red')\n\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(3178).Z,width:"461",height:"428"}))),(0,l.kt)("p",null,"and again"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# plot the data\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nax.scatter(df['x'], df['y'])\n\nx = np.linspace(0, 4, 5)\ny = 0.5*x + 1\ny_ = 0.5*df['x'] + 1\n\nresiduals = (y_ - df['y'])**2\nSE = np.sum(residuals)\nMSE = np.mean(residuals)\n\nax.plot(x,y, label=f'y= 0.5x + 1')\n\nax.plot([1,1],[1, 1.5], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.1, 1.5, '0.25')\nax.plot([1.5,1.5],[2.3, 1.75], color='red', linestyle='dotted', alpha=0.5)\nax.text(1.6, 2.25, '0.025')\nax.plot([2.5,2.5],[2.1, 2.25], color='red', linestyle='dotted', alpha=0.5)\nax.text(2.6, 2.12, '0.09')\nax.plot([3,3],[3, 2.5], color='red', linestyle='dotted', alpha=0.5)\nax.text(3.1, 2.4, '0.09')\nax.plot([4,4],[2.9, 3], color='red', linestyle='dotted', alpha=0.5)\nax.text(4.1, 2.6, '0.16')\n\nax.text(1.5, 1, f'\u2211={SE}\\nMSE={MSE}', fontsize=20, color='red')\n\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(4685).Z,width:"620",height:"428"}))),(0,l.kt)("p",null,"For simple linear regression, since we're only changing 2 parameters ($\\theta",(0,l.kt)("em",{parentName:"p"},"{1}$ the slope and the $\\theta"),"{0}$ y-intercept), we can plot the cost function as a 3d surface, and we can see that the cost function is a convex function, which means that it has a single global minimum, and we can use gradient descent to find it."),(0,l.kt)("p",null,"the plot would like a bowl shape, and the minimum value (the optimum values for $\\theta",(0,l.kt)("em",{parentName:"p"},"{0}$ and $\\theta"),"{1}$ is the bottom of the bowl."),(0,l.kt)("img",{alt:"cost function",src:"./assets/Surface-Plot-of-a-Two-Dimensional-Objective-Function.webp",width:"600"}),(0,l.kt)("p",null,"so what is gradient descent? and how does it work?"),(0,l.kt)("h2",{id:"gradient-descent"},"Gradient Descent"),(0,l.kt)("p",null,"gradient descent is used in many machine learning algorithms, not just linear regression. It's an optimization algorithm that helps us find the minimum of a function."),(0,l.kt)("p",null,"supposed you're on a mountain, it's dark, it's foggy, and you can only feel the slope of the ground below your feet. A good strategy to get down, is that you feel the ground and move it in the direction of the steepest slope. That is exactly what the gradient descent does."),(0,l.kt)("p",null,"$$\\Theta^{ next step} = \\Theta - \\eta . MSE(\\Theta) $$\nwhere:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"theta: the feature coefficients"),(0,l.kt)("li",{parentName:"ul"},"eta: the learning rate")),(0,l.kt)("p",null,"everytime you update the coefficients $\\Theta$, we get a new $\\hat{y}$, calculate the cost function, and calculate the gradient, and you move in the direction of the gradient, and you keep doing this until you reach the minimum. a point where the gradient is zero (or very close to zero). a value where the update theta values doesn't change."),(0,l.kt)("h2",{id:"using-scikit-learn"},"Using ",(0,l.kt)("inlineCode",{parentName:"h2"},"Scikit-Learn")),(0,l.kt)("h3",{id:"simple-linear-regression"},"Simple Linear Regression"),(0,l.kt)("p",null,"Scikit-learn abstracts all of the mathematical complexity away from us, and gives us a very simple API to work with."),(0,l.kt)("p",null,"This may not always be possible for all algorithms, or even desirable, which is why you need to understand the model you're using, to know it's limitations."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# linear regression\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndf = pd.DataFrame({'x': [1, 1.5, 2.5, 3, 4], 'y': [1, 2.3, 2.1, 3, 2.9]})\ndisplay(df)\n# create a linear regression model\nmodel = LinearRegression()\nmodel.fit(df[['x']], df['y'])\n\nmodel.coef_, model.intercept_\n")),(0,l.kt)(c,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.5</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.5</td>\n      <td>2.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>2.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"(array([0.55789474]), 0.9210526315789476)\n"))),(0,l.kt)("p",null,"That's is the model is trained, we can use it to make predictions."),(0,l.kt)("p",null,"Let's visualize the predictions."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df\n")),(0,l.kt)(c,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.5</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.5</td>\n      <td>2.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>2.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n'))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import matplotlib.pyplot as plt\n\n# plot the data points\nf, ax = plt.subplots(figsize=(5,5))\nax.scatter(df['x'], df['y'])\n\n# plot the regression line\nx = np.linspace(0, 4, 5)\ny = model.predict(x.reshape(-1,1))\nax.plot(x,y, label=f'y= {model.intercept_} + {model.coef_[0]}x')\n\n# calculate the predictions and residuals\ny_hat = model.predict(df[['x']]) # y_hat = model.coef_[0]*df['x'] + model.intercept_\nresiduals = (y_hat - df['y'])**2\nSE = np.sum(residuals)\nMSE = np.mean(residuals)\n\n# update the dataframe with an additional column for the predictions\ndf['y_hat'] = y_hat\ndisplay(df)\n\n# loop over the data points in the df\nfor i, row in df.iterrows():\n    ax.plot([row['x'], row['x']], [row['y'], row['y_hat']], color='red', linestyle='dotted', alpha=0.5)\n    y_midpoint = (row['y'] + row['y_hat'])/2\n    ax.text(row['x']+0.1, y_midpoint, f'{residuals[i]:.5f}')\n\nax.text(0, 2.5, f'\u2211={SE:.5f}\\nMSE={MSE:.5f}', fontsize=20, color='red')\n\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(c,{mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"/Users/gilanyym/.local/share/virtualenvs/IT4063C.github.io-cdKt1PoY/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>y_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.478947</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.5</td>\n      <td>2.3</td>\n      <td>1.757895</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.5</td>\n      <td>2.1</td>\n      <td>2.315789</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.594737</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>2.9</td>\n      <td>3.152632</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n')),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(5377).Z,width:"487",height:"428"}))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sns.scatterplot(x='Education', y='Income', data=income_df)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(2222).Z,width:"573",height:"432"}))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.linear_model import LinearRegression\n\n# create and train the model\nincome_model = LinearRegression()\nincome_model.fit(income_df[['Education']], income_df['Income'])\n\n# plot the data points\nsns.scatterplot(x='Education', y='Income', data=income_df)\n\n# plot the regression line\nx = np.linspace(10, 22, 5)\ny = income_model.coef_[0]*x + income_model.intercept_\n\nplt.plot(x,y, label=f'y= {income_model.intercept_} + {income_model.coef_[0]}x', color='red')\n\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(4032).Z,width:"571",height:"432"}))),(0,l.kt)("h3",{id:"multiple-linear-regression"},"Multiple Linear Regression"),(0,l.kt)("p",null,"ok let's try multiple linear regression, more than one feature."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.linear_model import LinearRegression\n\nincome_model = LinearRegression()\nincome_model.fit(income_df[['Education', 'Seniority']], income_df['Income'])\n\nincome_model.coef_, income_model.intercept_\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    (array([5.89555596, 0.17285547]), -50.08563875473381)\n"))),(0,l.kt)("p",null,"note how the coefficients have multiple values."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"## Prepare the data for Visualization\nimport numpy as np\n\nx_surf, y_surf = np.meshgrid(\n  np.linspace(income_df.Education.min(), income_df.Education.max(), 100),\n  np.linspace(income_df.Seniority.min(), income_df.Seniority.max(), 100)\n)\nsurfaceX = pd.DataFrame({'Education': x_surf.ravel(), 'Seniority': y_surf.ravel()})\npredictedIncomeForSurface=income_model.predict(surfaceX)\n\n## convert the predicted result in an array\npredictedIncomeForSurface=np.array(predictedIncomeForSurface)\npredictedIncomeForSurface\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    array([ 12.32703024,  13.01700126,  13.70697228, ..., 108.22241178,\n           108.9123828 , 109.60235383])\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Visualize the Data for Multiple Linear Regression\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfig = plt.figure(figsize=(20,10))\n### Set figure size\nax = fig.add_subplot(111, projection='3d')\nax.scatter(income_df['Education'],income_df['Seniority'],income_df['Income'],c='red', marker='o', alpha=0.5)\nax.plot_surface(x_surf, y_surf, predictedIncomeForSurface.reshape(x_surf.shape), color='b', alpha=0.3)\nax.set_xlabel('Education')\nax.set_ylabel('Seniority')\nax.set_zlabel('Income')\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(6282).Z,width:"792",height:"790"}))),(0,l.kt)("p",null,"and it doesn't matter how many additional features you add to the model, this would still hold. As long as the model we're predicting can fit this model.\nOnce we're past the 3d, we don't really know if the linear model is approproate for the data, since we can't visualize it. "),(0,l.kt)("p",null,"so as a machine learning professional, you'd be trying different models and algorithms, evaluating them, and then choosing the best one."),(0,l.kt)("p",null,"We will take next week about how we can evaluate our model, when we can't visualize it."),(0,l.kt)("h3",{id:"polynomial-regression"},"Polynomial Regression"),(0,l.kt)("p",null,"polynomial regression is a special case of multiple linear regression, where we're using polynomial features instead of linear features."),(0,l.kt)("p",null,"For a single feature $x$, we can create polynomial features up to degree $n$ as:"),(0,l.kt)("p",null,"$$x",(0,l.kt)("em",{parentName:"p"},"{0} = 1$$\n$$x"),"{1} = x$$\n$$x",(0,l.kt)("em",{parentName:"p"},"{2} = x^{2}$$\n$$x"),"{3} = x^{3}$$\n$$...$$\n$$x_{n} = x^{n}$$"),(0,l.kt)("p",null,"Let's visualize that."),(0,l.kt)("p",null,"As the model polynomial degree increases, the model complexity increases, and the model becomes more flexible. and that's why we can fit more complex data with polynomial regression."),(0,l.kt)("p",null,":::warn\nyou want to be careful, because if you use a polynomial model with a high degree, you can overfit the data.\n:::"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"x = np.linspace(-10, 10, 20)\nfirst_degreee_y = 20*x + 1\nsecond_degree_y = 10*x**2 + 2*x + 1\nthird_degree_y = 0.5*x**3 + x**2 + 2*x + 1\nfourth_degree_y = 0.1*x**4 + 0.5*x**3 + x**2 + 2*x + 1\n# visualize the data with different polynomial degrees\nf, ax = plt.subplots(figsize=(5,5))\nax.plot(x,first_degreee_y, label='1st degree')\nax.plot(x,second_degree_y, label='2nd degree')\nax.plot(x,third_degree_y, label='3rd degree')\nax.plot(x,fourth_degree_y, label='4th degree')\nax.legend()\nax.grid(alpha=0.2)\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(4271).Z,width:"455",height:"428"}))),(0,l.kt)("p",null,"Now mathematically, polynomial regression is the same as multiple linear regression, except that we're using polynomial features instead of linear features."),(0,l.kt)("p",null,"this was linear regression:\n$$\\hat{y} = h(x) = \\theta",(0,l.kt)("em",{parentName:"p"},"{0} + \\theta"),"{1}x",(0,l.kt)("em",{parentName:"p"},"{1} + \\theta"),"{2}x_{2}$$"),(0,l.kt)("p",null,"this is polynomial regression of the third degree for a single feature:\n$$\\hat{y} = h(x) =$$\n$$\\;\\theta",(0,l.kt)("em",{parentName:"p"},"{0} + $$\n$$\\;\\theta"),"{1",(0,l.kt)("em",{parentName:"p"},"{x1}}x"),"{1} + \\theta",(0,l.kt)("em",{parentName:"p"},"{2"),"{x1}}x",(0,l.kt)("em",{parentName:"p"},"{1}^{2} + \\theta"),"{3",(0,l.kt)("em",{parentName:"p"},"{x1}}x"),"{1}^{3} +$$\n$$\\;\\theta",(0,l.kt)("em",{parentName:"p"},"{1"),"{x2}}x",(0,l.kt)("em",{parentName:"p"},"{2} + \\theta"),"{2",(0,l.kt)("em",{parentName:"p"},"{x2}}x"),"{2}^{2} + \\theta",(0,l.kt)("em",{parentName:"p"},"{3"),"{x2}}x_{2}^{3} + ... $$"),(0,l.kt)("p",null,"For every feature x, you get multiple terms with different degrees. and you can see that the model is getting more complex as the degree increases."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"income2_df = pd.read_csv('data/position_salaries.csv')\n\nsns.scatterplot(x='Level', y='Salary', data=income2_df)\n\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(4473).Z,width:"567",height:"448"}))),(0,l.kt)("p",null,"If I was to fit it using linear regression, I would get a straight line, and if I was to fit it using polynomial regression, I would get a curved line."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"income2_model = LinearRegression()\nincome2_model.fit(income2_df[['Level']], income2_df['Salary'])\n")),(0,l.kt)(c,{center:!0,mdxType:"HTMLOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-html"},'<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: "\u25b8";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "\u25be";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter\'s `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" checked><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>\n'))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sns.scatterplot(x='Level', y='Salary', data=income2_df)\nx2 = np.linspace(1, 12, 20)\ny2 = income2_model.predict(x2.reshape(-1,1))\n\ny_hat = model.predict(income2_df[['Level']])\n\nresiduals = (y_hat - income2_df['Salary'])**2\nMSE = np.mean(residuals)\n\nplt.text(1, 800000, f'MSE={MSE:.5f}', fontsize=15, color='red')\n\nplt.plot(x2,y2, label=f'y= {income2_model.intercept_} + {income2_model.coef_[0]}x', color='red')\n\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    /Users/gilanyym/.local/share/virtualenvs/IT4063C.github.io-cdKt1PoY/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning:\n    \n    X does not have valid feature names, but LinearRegression was fitted with feature names\n    \n    /Users/gilanyym/.local/share/virtualenvs/IT4063C.github.io-cdKt1PoY/lib/python3.10/site-packages/sklearn/base.py:493: FutureWarning:\n    \n    The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n    Feature names unseen at fit time:\n    - Level\n    Feature names seen at fit time, yet now missing:\n    - x\n    \n    \n\n\n\n    \n    \n")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(9943).Z,width:"567",height:"448"}))),(0,l.kt)("p",null,"But if we use Polynomial Features, we transform the data to a higher dimension, and we can fit a linear model to it."),(0,l.kt)("p",null,"For example using a second degree polynomial:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\npoly_X= poly.fit_transform(income2_df[['Level']])\n\npolynomial_income2_model = LinearRegression()\npolynomial_income2_model.fit(poly_X, income2_df['Salary'])\n\nsns.scatterplot(x='Level', y='Salary', data=income2_df)\n\nx3 = np.linspace(1, 12, 20)\ny3 = polynomial_income2_model.predict(poly.fit_transform(x3.reshape(-1,1)))\ny_hat = polynomial_income2_model.predict(poly.fit_transform(income2_df[['Level']]))\n\nresiduals = (y_hat - income2_df['Salary'])**2\nMSE = np.mean(residuals)\n\nplt.text(1, 800000, f'MSE={MSE:.5f}', fontsize=15, color='red')\n\nplt.plot(x3,y3, color='red')\n\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(1904).Z,width:"567",height:"448"}))),(0,l.kt)("p",null,"using a third degree polynomial:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.preprocessing import PolynomialFeatures\n\npoly3 = PolynomialFeatures(degree=3)\npoly3_X= poly3.fit_transform(income2_df[['Level']])\n\npolynomial3_income2_model = LinearRegression()\npolynomial3_income2_model.fit(poly3_X, income2_df['Salary'])\n\nsns.scatterplot(x='Level', y='Salary', data=income2_df)\n\nx4 = np.linspace(1, 12, 20)\ny4 = polynomial3_income2_model.predict(poly3.fit_transform(x4.reshape(-1,1)))\ny_hat = polynomial3_income2_model.predict(poly3.fit_transform(income2_df[['Level']]))\n\nresiduals = (y_hat - income2_df['Salary'])**2\nMSE = np.mean(residuals)\n\nplt.text(1, 800000, f'MSE={MSE:.5f}', fontsize=15, color='red')\n\nplt.plot(x4,y4, color='red')\n\nplt.show()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(1367).Z,width:"567",height:"448"}))),(0,l.kt)("h3",{id:"multiple-polynomial-regression"},"Multiple Polynomial Regression"),(0,l.kt)("p",null,"Here I'm using the pipeline to transform the data, and then fit the model."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Visualize the same data using different polynomial degrees\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nfig = plt.figure(figsize=(20,10))\n### Set figure size\nax = fig.add_subplot(111, projection='3d')\nax.scatter(income_df['Education'],income_df['Seniority'],income_df['Income'],c='red', marker='o', alpha=0.5)\n\nfor degree in [1,2,3,4]:\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(income_df[['Education', 'Seniority']], income_df['Income'])\n    predictedIncomeForSurface=model.predict(surfaceX)\n    ax.plot_surface(x_surf, y_surf, predictedIncomeForSurface.reshape(x_surf.shape), alpha=0.3)\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("p",null,(0,l.kt)("img",{alt:"png",src:n(7517).Z,width:"792",height:"790"}))))}u.isMDXComponent=!0},2451:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_10_0-95f3600e510255771c98409bc75390e1.png"},9006:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_17_0-ad59a2d1628367db1ed226946655d101.png"},4238:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_19_0-ecb213577173f7ce482a6fdde8213e1b.png"},9618:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_22_0-e400efb2142bc19513c282a1b970c723.png"},285:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_26_0-b9ae7eec0c68c3c25b6dd19d2b96a010.png"},1132:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_28_0-b6c6e91a0b7d1a8f8bf817c737b23ab9.png"},2666:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_30_0-bbfd45325cfd949eb1e1b221ea9c1815.png"},3178:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_32_0-100397105a668379cd7c6a0e5863c8ab.png"},4685:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_34_0-6a2a0342c0ad4b9fa2c212efebea0677.png"},5377:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_41_2-7c126bf54cab46772228d259620fe547.png"},2222:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_42_0-f2b958686b4d3a52edd415c3803d9da6.png"},4032:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_43_0-f1bca68ec3e821f0144533469ffb30f3.png"},6282:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_48_0-7b6b3ac41ebd98d7df0ae67f777aac42.png"},4271:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_52_0-2a12f3395af4169c28fb4df234fdf2e9.png"},4473:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_54_0-405aa86f618392b335f45ce31f80f927.png"},9943:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_57_1-4058fb5cd9c0ec3042802070401a26f7.png"},1904:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_59_0-7e9c475caa96f9b3f7602d632e1a6057.png"},1367:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_61_0-08f6f98a45a38cd2e7dfe72411411f1c.png"},7517:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_63_0-efaa571ea778b1b7116994da3a78679b.png"},2838:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_6_0-ed154fddba8aed8d0080f6e6c40ab339.png"},5059:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/3d-income-6ba66e9b05e149837cc14921212adc89.gif"},7306:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/regression-types-0325dd427d49a02ebf94430100598c5d.png"}}]);