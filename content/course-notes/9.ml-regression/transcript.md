---
sidebar_position: 1
title: Regression - Transcript
draft: true
---


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->

In this module, we'll
# Machine Learning Process
1. Define the problem
2. Collect the data
3. Prepare the data
4. Evaluate the algorithms
5. Improve the results
6. Present the results

- start with the types of ML problems and the types of ML algorithms
- Frame the problem and look at the big picture
  - are we trying to get an estimate for the price
  - or just a category indicating (cheap, medium, expensive), in that case, accuracy is not important, and this could become a classification problem
- selecting a performance measure
  - RSME
  - MAE
  - both of theses are ways to measure the distance between two vectors (the vectors are the predictions and the labels)
- introduce cost function
- introduce the terminology and notations
- Splitting the data into training and test sets
- Computers don't generate truly random numbers, so we need to set the seed
  - https://www.statisticshowto.com/random-seed-definition/#:~:text=Generator%20in%20Excel.-,What%20is%20a%20Random%20Seed%3F,Henkemans%20%26%20Lee%2C%202001).
- Feature Scaling
  - https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35

- scikit-learn has:
  - estimators
  - transformers
  - predictors
  - They can be chained together using pipelines



```python
from sklearn import linear_model
import pandas as pd

X = [[0, 0], [1, 1], [2, 2]]
y = [0, 1, 2]
df = pd.DataFrame(X, columns=['x1', 'x2'])
df['y'] = y
display(df)

reg = linear_model.LinearRegression()
reg.fit(df[["x1", "x2"]], df['y']).coef_
```
    
<HTMLOutputBlock >


```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>
```





    array([0.5, 0.5])



</HTMLOutputBlock>


```python
from sklearn import datasets

diabetes = datasets.load_diabetes(as_frame=True)
diabetes.frame.head()

# diabetes.DESCR
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.038076</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>0.021872</td>
      <td>-0.044223</td>
      <td>-0.034821</td>
      <td>-0.043401</td>
      <td>-0.002592</td>
      <td>0.019907</td>
      <td>-0.017646</td>
      <td>151.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.001882</td>
      <td>-0.044642</td>
      <td>-0.051474</td>
      <td>-0.026328</td>
      <td>-0.008449</td>
      <td>-0.019163</td>
      <td>0.074412</td>
      <td>-0.039493</td>
      <td>-0.068332</td>
      <td>-0.092204</td>
      <td>75.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.085299</td>
      <td>0.050680</td>
      <td>0.044451</td>
      <td>-0.005670</td>
      <td>-0.045599</td>
      <td>-0.034194</td>
      <td>-0.032356</td>
      <td>-0.002592</td>
      <td>0.002861</td>
      <td>-0.025930</td>
      <td>141.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.089063</td>
      <td>-0.044642</td>
      <td>-0.011595</td>
      <td>-0.036656</td>
      <td>0.012191</td>
      <td>0.024991</td>
      <td>-0.036038</td>
      <td>0.034309</td>
      <td>0.022688</td>
      <td>-0.009362</td>
      <td>206.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005383</td>
      <td>-0.044642</td>
      <td>-0.036385</td>
      <td>0.021872</td>
      <td>0.003935</td>
      <td>0.015596</td>
      <td>0.008142</td>
      <td>-0.002592</td>
      <td>-0.031988</td>
      <td>-0.046641</td>
      <td>135.0</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>


```python
from zlib import crc32
import numpy as np

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

print(crc32(np.int64('1')) & 0xffffffff)
print(crc32(np.int64(4294967296)) & 0xffffffff)
print(0xffffffff)
print(2**32)
```

<CodeOutputBlock lang="python">

```
    2844319735
    3718166540
    4294967295
    4294967296
```

</CodeOutputBlock>

# Regression
This week we'll talk about one supervised learning technique and algorithm. which is used to predict (continuous) values.
it's a statistical tool for modeling the relationship between a dependent variable (the target) and one or more independent variables (explanatory variable).
that is linear regression.

we'll start with a simple linear regression model, and learn how the same model can be applied for multiple linear regression, and finally we'll talk about polynomial regression.

here's the agenda for what we'll talk about in this module:
## outline
- introduction
- notations
- simple linear regression
  - performance measure AKA cost function (RMSE)
  - gradient descent
- multiple linear regression
- polynomial regression
- linear regression with scikit-learn
- effects of features on the model
- Feature Engineering
  - how to choose the features
  - how to deal with categorical features (ordinal, nominal)




## Module Overview
We've talked about what machine learning is, the different problems it can help us solve, and the different types of machine learning algorithms. In this module, we'll talk about one of the supervised learning techniques, typically used for predicting numerical values. We'll start with a simple linear regression model, then we'll see how that model can be applied for multiple linear regression problems as well, and finally we'll talk about polynomial regression.

Our agenda for this module is as follows:
- Introduction: we'll introduce the problem of regression, what it means, what are the types of regression problems.
- Notations: we'll introduce the terminology and notations we'll use throughout this module. These are notations that you'll find in most machine learning books and courses.
- Linear Regression - how to train a model.
  - Performance Measures and Cost Functions: this is what tells us how good our model is.
  - Gradient Descent: this is the algorithm we'll use to train our model. it uses the information from the cost function to update your model incrementally.
- Implement the model using scikit-learn
- a bit on Feature Engineering
  - how to evaluate your features
  - how to deal with categorical features (ordinal, nominal)
- we'll do a quick recap showing how we can build models using scikit-learn

Let's get started.

## Introduction: What is Regression? Types?
Linear Regression and Polynomial Regression are supervised regression algorithms.

Supervised meaning we use labeled data to train the model.
Regression meaning we predict numerical values instead of categories of classes.

Regression models finds relationships between one or more independent variables, or explanatory variables, and some target variable or dependent variable; as it it depends on the values of your features. The target variable is the value we're trying to predict.

Depending on the shape of the model that would fit the data, we call the model either linear or polynomial. 
If it's simple enough that we can represent it by a straight line, or a plane, a surface we call it a linear model. If it's more complex, where it needs to represented by a curved line or surface we call it a polynomial model.

Keep in mind, it's not always clear from the get-go whether a model is linear or polynomial. It's something we'll have to figure out as we go along.
You experiment with different models, and different algorithms, and you'll see which one works best for your data, or performs best on your data.

For linear regression, we have a simple linear regression model, and a multiple linear regression model. The simple linear regression model is used when we have one independent variable, and the multiple linear regression model is used when we have more than one independent variable.
We can visualize the simple linear regression model as a straight line, and the multiple linear regression model up-to 2 variables as a plane, or a surface. it's a bit more difficult to visualize the multiple linear regression model with more than 2 variables. and at the point we just trust the math and the algorithms that we were able to generalize from the 2D case to the 3D case.

Just so we can start with the end in mind, here's the desired outcome of regression models.

Imagine, you have this dataset that shows 30 observation of years of education, seniority level, and and income



```python
# import csv file
import pandas as pd

income_df = pd.read_csv('data/income2.csv')
income_df.head(6)
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Education</th>
      <th>Seniority</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>21.586207</td>
      <td>113.103448</td>
      <td>99.917173</td>
    </tr>
    <tr>
      <th>1</th>
      <td>18.275862</td>
      <td>119.310345</td>
      <td>92.579135</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12.068966</td>
      <td>100.689655</td>
      <td>34.678727</td>
    </tr>
    <tr>
      <th>3</th>
      <td>17.034483</td>
      <td>187.586207</td>
      <td>78.702806</td>
    </tr>
    <tr>
      <th>4</th>
      <td>19.931034</td>
      <td>20.000000</td>
      <td>68.009922</td>
    </tr>
    <tr>
      <th>5</th>
      <td>18.275862</td>
      <td>26.206897</td>
      <td>71.504485</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

If we EDA this data, we can see that there is a positive correlation between years of education and income, and a positive correlation between seniority level and income.


```python
# Visualize the correlation matrix
import seaborn as sns
import matplotlib.pyplot as plt

income_corr = income_df.corr()
sns.heatmap(income_corr, annot=True, cmap='coolwarm')
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_10_0.png)
    
```

</CodeOutputBlock>

It's a quite strong correlation between education and income (0.9), and a slightly weaker correlation between seniority level and income (0.52).

Which means we can create a single linear regression model the predicts income based on years of education. or a multiple linear regression that also includes the seniority level.

let's visualize the data using a scatter plot. We can clearly see that there is a positive correlation between years of education and income. Linear Regression will allow us to find the best fit lin or best fit model that will allow us to predict the income of a person given their years of education.


```python
# Visualize a scatter plot between income and education
sns.scatterplot(x='Education', y='Income', data=income_df)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_12_0.png)
    
```

</CodeOutputBlock>

But let's try to get, seniority level in the mix. Visualize it an 3D scatter plot.


```python
# visualize a 3d scatter plot between Education, Seniority, and Income using mpl_toolkits
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(5,5))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(income_df['Education'], income_df['Seniority'], income_df['Income'])
ax.set_xlabel('Education')
ax.set_ylabel('Seniority')
ax.set_zlabel('Income')
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_14_0.png)
    
```

</CodeOutputBlock>

ok we kind of see the relationship, but I think an interactive plot will be better. so let's recreate this using plotly.


```python
# visualize a 3d scatter plot between Education, Seniority, and Income using plotly
import plotly.express as px


fig = px.scatter_3d(income_df, x='Education', y='Seniority', z='Income')
fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))
fig.show()
```

<CodeOutputBlock lang="python">

```

```

</CodeOutputBlock>

![3d scatter plot](assets/3d-income.gif)

- Look at the different relationships between the 3 variables.
- visualize them on 2d scatter plots
- show that a surface can cut through that.

we can see that we can get a surface model that can cut through the data points, and this is the best fit model. Once we start using more than one variable as independent variables, or explanatory variables, that's multiple linear regression.

Finding that line, finding that surface, finding that curve, that's the goal of regression models. once you have more than 2 independent variables, it's not easy to visualize the model, but the math will still hold.


Now that we know what we want to achieve, let's talk about the model and its notation.

See you in the next video.


## Model and Notations
This is a very short video, but it's very important. 
It introduces the notations that you'll find in a lot of machine learning algorithms, and in other machine learning courses and books.
so it's important to understand them.

Before we talk about the model notation, let's talk about the data notations.

Every dataset we work with has a number (m) of (records, observations, signals, instances, data point), these are all synonyms.
Each of those observations ia a vector of (features, independent variables, explanatory variables, predictors, dimensions, attributes) (x) and a for labelled data, or data we use in supervised learning we also have a target (y). These are also called targets, dependent variables, or a responses, maybe classes, or a categories if it's a class.
the number of features is (n). 

so for this dataset, we can say that:
- we have 30 records, m = 30
- we have 2 features, n = 2
- features are years of education, and seniority level
- target is income

Now let's forget about machine learning for a second, and talk about something we all know from high school math, the equation of a line.

$$y = mx + b$$

where:
- m: slope
- c: y-intercept or the bias term, or the value of y when x = 0

sounds familiar? if we you want to get the y value, you multiply the x value by the slope, and add the y-intercept.


```python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-5, 5, 11)

def plot_line(y, color):
  y_ = eval(y)
  plt.plot(x, y_, label=f'y={y}', marker='o', markersize=5, color=color)
  plt.legend(loc='best')
  plt.xlabel('x')
  plt.ylabel('y')

print(x)
```

<CodeOutputBlock lang="python">

```
    [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]
```

</CodeOutputBlock>

so we are able to represent a line using those 2 parameters.


```python
plot_line(y='x', color='red')
# plot_line(y='3*x+5', color='blue')
# plot_line(y='-2*x+4', color='green')
# plot_line(y='2*x', color='purple')

plt.grid()
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_22_0.png)
    
```

</CodeOutputBlock>

ok so we mentioned for simple linear regression, the desired model, the desired outcome is to draw a line. so essentially, we're trying to figure out what the value of m and b are, so that we can draw a line that fits our data.

it's the same formula but the notation is different. we say
$$\hat{y} = h(x) = \theta_{0} + \theta_{1}x$$

where:
- $\hat{y}$: the predicted value, also called the hypothesis function $h(x)$
- $\theta_{0}$: the bias term, or the y-intercept
- $\theta_{1}$: the slope of the line

it's as if the $\theta_{0}$ was multiplied by $x^{0} = 1$

ok but let's generalize that so we can use to represent a plane, or a surface.

$$\hat{y} = h(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2}$$

this is as if x2 was the z-axis, and we're trying to draw a plane that satisfies the equation.


```python
fig = plt.figure()

ax = fig.add_subplot(111,projection='3d')

x1, x2 = np.meshgrid(range(10), range(10))

y_hat = (9 - x1 - x2)

ax.plot_surface(x1, x2, y_hat, alpha=0.5)

plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_24_0.png)
    
```

</CodeOutputBlock>

so that would be your multiple linear regression model. so the most generalized form of this equation is:

$$\hat{y} = h(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{n}x_{n}$$

or the vectorized form of this would be:

$$\hat{y} = h(x) = \Theta^{T}X$$

if you're not familiar with the vectorized form, don't worry about it. it's just a way to represent the same equation in a more compact way. Unless you're creating your own implementation of the algorithm, you don't need to worry about how all the math is done, but it's just important to understand how the model works, so you understand the results you're getting.

so to summarize the notations:
- $X$: the matrix of features, or the matrix of independent variables
- $\Theta$: the vector of parameters, or the vector of coefficients, the values we're adjusting to fit the model.
- $\hat{y}$: the predicted value, also called the hypothesis function $h(x)$

now that we know the model, let's talk about how we can train the model. how we can find the best values for $\Theta$.

## Linear Regression - Performance Measure/Cost Function
In simple linear regression, we aim to get a a line that can represent the data we have as best as possible. So we need to define what "best" means.
why do we say that this line here is better than this line here?


```python
# a pandas dataframe with 2 columns: x and y
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

x = [1, 1.5, 2.5, 3, 4]
y = [1, 2.3, 2.1, 3, 2.9]

df = pd.DataFrame({'x': x, 'y': y})
# plot the data

# Scatter plot of the dots
f, ax = plt.subplots(figsize=(5,5))
ax.scatter(df['x'], df['y'], s=60)

# Plot an arbitrary line
x = np.linspace(0, 4, 11)
y_bad = 2.2+ -x
y_good = x + 0.5
y_better = 0.55789474*x + 0.92105263
ax.plot(x,y_bad, label=f'bad: y=2.2 - x', color='red')
ax.plot(x,y_good, label=f'good: y=0.5 + x', color='orange')
ax.plot(x,y_better, label=f'better: y= 0.92 + 0.558 x', color='green')
ax.legend(loc='best')

ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_27_0.png)
    
```

</CodeOutputBlock>

You probably remember that to calculate the line slope, it was the change in y divided by the change in x.
so we were always able to draw a line by knowing 2 points on the line.
$$y = \frac{y_{2} - y_{1}}{x_{2} - x_{1}}x + b$$

this always got us the perfect line, but in real life, we don't have the perfect data. we have noisy data, and we need to find the best line that fits the data we have. we just don't have 2 lines that we can connect.

ok the next best thing is to eyeball it. While we can eyeball it in 2D, it's not easy to eyeball it in 3D. and impossible to eyeball in 4D or more. Plus we need an objective way to measure how good the line is. and that's where the cost function or performance measures comes in.

Now there are many different performance measures, the most common of which is that we get the mean of the squared errors or the squared residuals. 

What does that mean? it means that we take the difference between the actual value and the predicted value, and we square it (so we can lose the negative sign). and we do that for every data point, and we get the mean of all those values.

$$MSE = \frac{1}{m}\sum_{i=1}^{m}(y_{i} - \hat{y_{i}})^{2}$$

The smaller the cost function, the better the model. so we want to find the model, we want to find the line or the surface that minimizes the mean squared error value.

Let's see an example here. using this same data here.



```python
import pandas as pd

df = pd.DataFrame([[1, 1], [1.5, 2.3], [2.5, 2.1], [3, 3], [4, 2.9]], columns=['x', 'y'])
df
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.5</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.5</td>
      <td>2.1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4.0</td>
      <td>2.9</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

we'll just start with a random line, any line, 


```python
# plot the data
import matplotlib.pyplot as plt

# Scatter plot of the dots
f, ax = plt.subplots(figsize=(5,5))
ax.scatter(df['x'], df['y'])

# Plot an arbitrary line
x = np.linspace(0, 4, 11)
y = 0*x + 2.2
ax.plot(x,y, label=f'y=2.25')

ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_31_0.png)
    
```

</CodeOutputBlock>

and then calculate the distance between the line and the data points.


```python
# plot the data
import matplotlib.pyplot as plt

# Scatter plot of the dots
f, ax = plt.subplots(figsize=(5,5))
ax.scatter(df['x'], df['y'])

# Plot an arbitrary line
x = np.linspace(0, 4, 11)
y = 0*x + 2.2
ax.plot(x,y, label=f'y=2.25')

# Residuals
ax.plot([1,1],[1, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.1, 1.5, '1.2')
ax.plot([1.5,1.5],[2.3, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.6, 2.25, '-0.1')
ax.plot([2.5,2.5],[2.1, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(2.6, 2.12, '0.1')
ax.plot([3,3],[3, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(3.1, 2.4, '-0.8')
ax.plot([4,4],[2.9, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(4.1, 2.6, '-0.7')

ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_33_0.png)
    
```

</CodeOutputBlock>

Then, we square the distance, and we sum all the squared distances, 


```python
# plot the data
import matplotlib.pyplot as plt

f, ax = plt.subplots(figsize=(5,5))
ax.scatter(df['x'], df['y'])

x = np.linspace(0, 4, 5)
y = 0*x + 2.2
y_ = 0*df['x'] + 2.2

residuals = (y_ - df['y'])**2
SE = np.sum(residuals)
MSE = np.mean(residuals)

ax.plot(x,y, label=f'y=2.25')

ax.plot([1,1],[1, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.1, 1.5, '1.44')
ax.plot([1.5,1.5],[2.3, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.6, 2.25, '0.01')
ax.plot([2.5,2.5],[2.1, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(2.6, 2.12, '0.01')
ax.plot([3,3],[3, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(3.1, 2.4, '0.64')
ax.plot([4,4],[2.9, 2.2], color='red', linestyle='dotted', alpha=0.5)
ax.text(4.1, 2.6, '0.49')

ax.text(2, 1.5, f'∑={SE}\nMSE={MSE}', fontsize=20, color='red')

ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_35_0.png)
    
```

</CodeOutputBlock>

$$MSE = \frac{1}{m}\sum_{i=1}^{m}(y_{i} - \hat{y_{i}})^{2}$$
We call this the loss function, or the cost function. and we want to minimize this function.

Then we change the parameters $\Theta$ , and we calculate the loss function again, and we repeat this process until we get the minimum value.

note here, this is the Theta matrix, as in all the parameters. so we're changing all the parameters at the same time.



```python
# plot the data
import matplotlib.pyplot as plt

f, ax = plt.subplots(figsize=(5,5))
ax.scatter(df['x'], df['y'])

x = np.linspace(0, 4, 5)
y = x + 0.5
y_ = df['x'] + 0.5

residuals = (y_ - df['y'])**2
SE = np.sum(residuals)
MSE = np.mean(residuals)

ax.plot(x,y, label=f'y=0.5 + x')

ax.plot([1,1],[1, 1.5], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.1, 1.5, '0.25')
ax.plot([1.5,1.5],[2.3, 2], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.6, 2.25, '0.09')
ax.plot([2.5,2.5],[2.1, 3], color='red', linestyle='dotted', alpha=0.5)
ax.text(2.6, 2.12, '0.81')
ax.plot([3,3],[3, 3.5], color='red', linestyle='dotted', alpha=0.5)
ax.text(3.1, 2.4, '0.25')
ax.plot([4,4],[2.9, 4.5], color='red', linestyle='dotted', alpha=0.5)
ax.text(4.1, 2.6, '2.56')

ax.text(1.5, .5, f'∑={SE}\nMSE={MSE}', fontsize=20, color='red')

ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_37_0.png)
    
```

</CodeOutputBlock>

and again


```python
# plot the data
import matplotlib.pyplot as plt

f, ax = plt.subplots(figsize=(5,5))
ax.scatter(df['x'], df['y'])

x = np.linspace(0, 4, 5)
y = 0.5*x + 1
y_ = 0.5*df['x'] + 1

residuals = (y_ - df['y'])**2
SE = np.sum(residuals)
MSE = np.mean(residuals)

ax.plot(x,y, label=f'y= 0.5x + 1')

ax.plot([1,1],[1, 1.5], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.1, 1.5, '0.25')
ax.plot([1.5,1.5],[2.3, 1.75], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.6, 2.25, '0.025')
ax.plot([2.5,2.5],[2.1, 2.25], color='red', linestyle='dotted', alpha=0.5)
ax.text(2.6, 2.12, '0.09')
ax.plot([3,3],[3, 2.5], color='red', linestyle='dotted', alpha=0.5)
ax.text(3.1, 2.4, '0.09')
ax.plot([4,4],[2.9, 3], color='red', linestyle='dotted', alpha=0.5)
ax.text(4.1, 2.6, '0.16')

ax.text(1.5, 1, f'∑={SE}\nMSE={MSE}', fontsize=20, color='red')

ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_39_0.png)
    
```

</CodeOutputBlock>


so again:
- measure error
- Square it
- sum it

you could actually create a 3D plot of the loss function, and you can see that the loss function is a bowl shape, and the minimum value is the bottom of the bowl.
for every $\theta_{0}$ and $\theta_{1}$, there's a value for the loss function.

<img alt="cost function" src="./assets/Surface-Plot-of-a-Two-Dimensional-Objective-Function.webp" width="600" />

ok but so far, we're updating those numbers randomly and manually. There's nothing automatic about it. so how do we automate this process?
how can we make sure we're updating the parameters in the right direction? the direction that will minimize the loss function?

That is the gradient descent algorithm.

## Gradient Descent
In this video, we'll talk about gradient descent, which is a very important algorithm in machine learning. It's used in many machine learning algorithms, not just linear regression.
it's an optimization algorithm that helps us find the minimum of a function. In our case, we want to find the minimum of the loss function.

supposed you're on a mountain, it's dark, it's foggy, and you can only feel the slope of the ground below your feet. A good strategy to get down, is that you feel the ground and move it in the direction of the steepest slope. That is exactly what the gradient descent does.

For any of your feature coefficients, you start somewhere and then you calculate the gradient of the slope, and then you move in the direction of the gradient, and then you calculate the gradient again, and then you move in the direction of the gradient, and it keeps doing this until it reaches the minimum.

$$\Theta^{ next step} = \Theta - \eta . MSE(\Theta) $$
where:
- theta: the feature coefficients
- eta: the learning rate

I'm presenting this formula in a vectorized form, but you can also write it in a loop form.
I think going over the math behind this formula is a bit out of scope for this course, but I'll leave a link to a video that explains it in detail.

anyway every coefficient is updated simultaneously, so you don't update one coefficient, and then update the other coefficient, and then update the other coefficient, and so on. you update all the coefficients simultaneously.

everytime you update the coefficients, you calculate the cost function, and calculate the gradient, and you move in the direction of the gradient, and you keep doing this until you reach the minimum, a point where the gradient is zero (or very close to zero). a value where the update theta values doesn't change.

In a way, this very simple problem seems that it'll take a lot of coding to solve it, but thankfully, a library called scikit-learn has a class called LinearRegression that does all the heavy lifting for us. 

This under the hood introduction to linear regression is important because it helps you understand the results you're getting, and it helps you understand the limitations of the model. and how your data and feature engineering process could affect the results you're getting.

and we'll see later in the module.

## Linear Regression in Scikit-Learn
Talk is cheap, let's see some code.
While there was a lot of math behind how linear regression models work, Scikit-learn abstracts all of that away from us, and gives us a very simple API to work with.
That may not always be possible, or even desirable, which is why you need to make sure that you understand enough about the model you're using, so you can make the right decisions.

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df['x'], df['y'])
```


```python
# linear regression
from sklearn.linear_model import LinearRegression
import pandas as pd

df = pd.DataFrame({'x': [1, 1.5, 2.5, 3, 4], 'y': [1, 2.3, 2.1, 3, 2.9]})
display(df)
# create a linear regression model
model = LinearRegression()
model.fit(df[['x']], df['y'])

model.coef_, model.intercept_
```
    
<HTMLOutputBlock >


```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.5</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.5</td>
      <td>2.1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4.0</td>
      <td>2.9</td>
    </tr>
  </tbody>
</table>
</div>
```





    (array([0.55789474]), 0.9210526315789476)



</HTMLOutputBlock>


```python
# plot the data
import matplotlib.pyplot as plt

f, ax = plt.subplots(figsize=(5,5))
ax.scatter(df['x'], df['y'])

x = np.linspace(0, 4, 5)
y = model.coef_[0]*x + model.intercept_
y_ = model.coef_[0]*df['x'] + model.intercept_

residuals = (y_ - df['y'])**2
SE = np.sum(residuals)
MSE = np.mean(residuals)

ax.plot(x,y, label=f'y= {model.intercept_} + {model.coef_[0]}x')

ax.plot([1,1],[1, 1.478947], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.1, 1.5, '0.229391')
ax.plot([1.5,1.5],[2.3, 1.757895], color='red', linestyle='dotted', alpha=0.5)
ax.text(1.6, 2.25, '0.293878')
ax.plot([2.5,2.5],[2.1, 2.315789], color='red', linestyle='dotted', alpha=0.5)
ax.text(2.6, 2.12, '0.046565')
ax.plot([3,3],[3, 2.594737], color='red', linestyle='dotted', alpha=0.5)
ax.text(3.1, 2.4, '0.164238')
ax.plot([4,4],[2.9, 3.152632], color='red', linestyle='dotted', alpha=0.5)
ax.text(4.1, 2.6, '0.063823')

ax.text(1.5, 1, f'∑={SE}\nMSE={MSE}', fontsize=20, color='red')

ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_44_0.png)
    
```

</CodeOutputBlock>

That's it really. LinearRegression abstracted away all the math. you don't even need to set the learning rate. 

if you have an input and want to get the predicted value, we can use the coefficient and the intercept and create the equation for the line, and then plug in the input value to get the predicted value.

too much work, I thought so too. so we can use the predict method to get the predicted value.


```python
model.predict([[3.5]])
```

<CodeOutputBlock lang="python">

```
    /Users/gilanyym/.local/share/virtualenvs/IT4063C.github.io-cdKt1PoY/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning:
    
    X does not have valid feature names, but LinearRegression was fitted with feature names
    





    array([2.87368421])
```

</CodeOutputBlock>

now I'm skipping a really important step here which is splitting the data into training and test sets. but I'll talk about in a later.
Also, I'm not doing any feature engineering here, but I'll talk about that in a later video.

so let's apply that to some data. and we're back to the income dataset.


```python
sns.scatterplot(x='Education', y='Income', data=income_df)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_49_0.png)
    
```

</CodeOutputBlock>


```python
from sklearn.linear_model import LinearRegression

income_model = LinearRegression()
income_model.fit(income_df[['Education']], income_df['Income'])

income_model.coef_, income_model.intercept_
```

<CodeOutputBlock lang="python">

```
    (array([6.38716122]), -41.91661220978743)
```

</CodeOutputBlock>


```python
sns.scatterplot(x='Education', y='Income', data=income_df)
x = np.linspace(10, 22, 5)
y = income_model.coef_[0]*x + income_model.intercept_
y_ = income_model.coef_[0]*df['x'] + income_model.intercept_

residuals = (y_ - df['y'])**2
SE = np.sum(residuals)
MSE = np.mean(residuals)

plt.plot(x,y, label=f'y= {income_model.intercept_} + {income_model.coef_[0]}x', color='red')

plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_51_0.png)
    
```

</CodeOutputBlock>

ok let's try multiple linear regression, more than one feature. Let's do that in the next video.

## Multiple Linear Regression


```python
from sklearn.linear_model import LinearRegression

income_model = LinearRegression()
income_model.fit(income_df[['Education', 'Seniority']], income_df['Income'])

income_model.coef_, income_model.intercept_
```

<CodeOutputBlock lang="python">

```
    (array([5.89555596, 0.17285547]), -50.08563875473381)
```

</CodeOutputBlock>


```python
## Prepare the data for Visualization
import numpy as np

x_surf, y_surf = np.meshgrid(
  np.linspace(income_df.Education.min(), income_df.Education.max(), 100),
  np.linspace(income_df.Seniority.min(), income_df.Seniority.max(), 100)
)
surfaceX = pd.DataFrame({'Education': x_surf.ravel(), 'Seniority': y_surf.ravel()})
predictedIncomeForSurface=income_model.predict(surfaceX)

## convert the predicted result in an array
predictedIncomeForSurface=np.array(predictedIncomeForSurface)
predictedIncomeForSurface
```

<CodeOutputBlock lang="python">

```
    array([ 12.32703024,  13.01700126,  13.70697228, ..., 108.22241178,
           108.9123828 , 109.60235383])
```

</CodeOutputBlock>


```python
# Visualize the Data for Multiple Linear Regression
from mpl_toolkits.mplot3d import Axes3D


fig = plt.figure(figsize=(20,10))
### Set figure size
ax = fig.add_subplot(111, projection='3d')
ax.scatter(income_df['Education'],income_df['Seniority'],income_df['Income'],c='red', marker='o', alpha=0.5)
ax.plot_surface(x_surf, y_surf, predictedIncomeForSurface.reshape(x_surf.shape), color='b', alpha=0.3)
ax.set_xlabel('Education')
ax.set_ylabel('Seniority')
ax.set_zlabel('Income')
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_55_0.png)
    
```

</CodeOutputBlock>


```python
# using plotly, plot a scatter plot of the data and the regression surface
import plotly.express as px
import plotly.graph_objects as go

fig = px.scatter_3d(income_df, x='Education', y='Seniority', z='Income')
fig.add_traces(
    go.Surface(
        x=x_surf,
        y=y_surf,
        z=predictedIncomeForSurface.reshape(x_surf.shape),
        opacity=0.5,
        showscale=False
    )
)
fig.show()
```

<CodeOutputBlock lang="python">

```

```

</CodeOutputBlock>

and it doesn't matter how many additional features you add to the model, this would still hold. As long as the model we're predicting can fit this model. 
Once we're past the 3d, we don't really know if the linear model is approproate for the data, since we can't visualize it. 
so as a machine learning professional, you'd be trying different models and algorithms, evaluating them, and then choosing the best one.

# Polynomial Regression
polynomial regression is a special case of multiple linear regression, where we're using polynomial features instead of linear features. as in the data is usually can't be described by a straight line or a plane, but by a curve, or a curved surface.

You know from before that as the model polynomial degree increases, the model complexity increases, and the model becomes more flexible. and that's why we can fit more complex data with polynomial regression.

with a single feature x, if I use polynomial model, I'm able to fit a more flexible model. but you want to be careful, because if you use a polynomial model with a high degree, you can overfit the data.




```python
x = np.linspace(-10, 10, 20)
first_degreee_y = 20*x + 1
second_degree_y = 10*x**2 + 2*x + 1
third_degree_y = 0.5*x**3 + x**2 + 2*x + 1
fourth_degree_y = 0.1*x**4 + 0.5*x**3 + x**2 + 2*x + 1
# visualize the data with different polynomial degrees
f, ax = plt.subplots(figsize=(5,5))
ax.plot(x,first_degreee_y, label='1st degree')
ax.plot(x,second_degree_y, label='2nd degree')
ax.plot(x,third_degree_y, label='3rd degree')
ax.plot(x,fourth_degree_y, label='4th degree')
ax.legend()
ax.grid(alpha=0.2)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_59_0.png)
    
```

</CodeOutputBlock>

Now mathematically, polynomial regression is the same as multiple linear regression, except that we're using polynomial features instead of linear features.

this was linear regression:
$$\hat{y} = h(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2}$$

this is polynomial regression:
$$\hat{y} = h(x) = \theta_{0} + \theta_{1_{x1}}x_{1} + \theta_{2_{x1}}x_{1}^{2} + \theta_{3_{x1}}x_{1}^{3} + \theta_{1_{x2}}x_{2} + \theta_{2_{x2}}x_{2}^{2} + \theta_{3_{x2}}x_{2}^{3} + ... $$

For every feature x, you get multiple terms with different degrees. and you can see that the model is getting more complex as the degree increases.


```python
income2_df = pd.read_csv('data/position_salaries.csv')
income2_df
sns.scatterplot(x='Level', y='Salary', data=income2_df)
plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_61_0.png)
    
```

</CodeOutputBlock>

If I was to fit it using linear regression, I would get a straight line, and if I was to fit it using polynomial regression, I would get a curved line.


```python
income2_model = LinearRegression()
income2_model.fit(income2_df[['Level']], income2_df['Salary'])

income2_model.coef_, income2_model.intercept_
```

<CodeOutputBlock lang="python">

```
    (array([85637.00234192]), -211689.6955503513)
```

</CodeOutputBlock>


```python
sns.scatterplot(x='Level', y='Salary', data=income2_df)
x = np.linspace(1, 12, 20)
y = income2_model.coef_[0]*x + income2_model.intercept_
y_ = income2_model.coef_[0]*income2_df['Level'] + income2_model.intercept_

residuals = (y_ - income2_df['Salary'])**2
SE = np.sum(residuals)
MSE = np.mean(residuals)

plt.plot(x,y, label=f'y= {income_model.intercept_} + {income_model.coef_[0]}x', color='red')

plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_64_0.png)
    
```

</CodeOutputBlock>


```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
poly_X= poly.fit_transform(income2_df[['Level']])
poly_X.shape
polynomial_income2_model = LinearRegression()
polynomial_income2_model.fit(poly_X, income2_df['Salary'])

sns.scatterplot(x='Level', y='Salary', data=income2_df)

x = np.linspace(1, 12, 20)
y = polynomial_income2_model.predict(poly.fit_transform(x.reshape(-1,1)))
y_ = income2_model.coef_[0]*income2_df['Level'] + income2_model.intercept_

residuals = (y_ - income2_df['Salary'])**2
SE = np.sum(residuals)
MSE = np.mean(residuals)

plt.plot(x,y, label=f'y= {income_model.intercept_} + {income_model.coef_[0]}x', color='red')

plt.show()
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_65_0.png)
    
```

</CodeOutputBlock>

try the above using different degrees, and you'll see that the model complexity increases as the degree increases.

Let's do it again using the the income dataset, multiple polynomial features, and see how it looks.

I will be introducing a tool from scikit-learn called pipeline, which is a very useful tool for preprocessing data and training models.


```python
# Visualize the same data using different polynomial degrees
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

fig = plt.figure(figsize=(20,10))
### Set figure size
ax = fig.add_subplot(111, projection='3d')
ax.scatter(income_df['Education'],income_df['Seniority'],income_df['Income'],c='red', marker='o', alpha=0.5)

for degree in [1,2,3,4]:
    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
    model.fit(income_df[['Education', 'Seniority']], income_df['Income'])
    predictedIncomeForSurface=model.predict(surfaceX)
    ax.plot_surface(x_surf, y_surf, predictedIncomeForSurface.reshape(x_surf.shape), alpha=0.3)
```

<CodeOutputBlock lang="python">

```
    
![png](_transcript_files/output_68_0.png)
    
```

</CodeOutputBlock>
